{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6211788,"sourceType":"datasetVersion","datasetId":3304856}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\nimport json\nimport copy\nimport random\nimport time\nimport torch\nfrom torch import nn, cuda, optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.init as init\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nif device == 'cuda':\n    torch.cuda.empty_cache()\nprint(device)\nfrom itertools import islice\n\ntry:\n    from torchcrf import CRF\nexcept:\n    !pip install pytorch-crf\n    from torchcrf import CRF\n\nfrom transformers import (\n    BertModel,\n    BertForTokenClassification,\n    BertTokenizerFast,\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    get_linear_schedule_with_warmup\n)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    f1_score,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    precision_recall_fscore_support,\n    classification_report\n)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef generate_random_seed():\n    return random.randint(1, 1000)\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-28T16:40:33.982457Z","iopub.execute_input":"2023-08-28T16:40:33.982811Z","iopub.status.idle":"2023-08-28T16:40:34.002767Z","shell.execute_reply.started":"2023-08-28T16:40:33.982783Z","shell.execute_reply":"2023-08-28T16:40:34.001733Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda\n/kaggle/input/unit-segmentation-lstm-transformers/we.csv\n/kaggle/input/unit-segmentation-lstm-transformers/mix1.csv\n/kaggle/input/unit-segmentation-lstm-transformers/pe.csv\n/kaggle/input/unit-segmentation-lstm-transformers/abam.csv\n/kaggle/input/unit-segmentation-lstm-transformers/mix2.csv\n/kaggle/input/unit-segmentation-lstm-transformers/ug.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"configuration = {\n    'train_tasks': ['pe', 'we', 'abam'],\n    'task_weighting': 'dwa', # dwa or equal\n    'test': ['pe', 'we', 'abam', 'ug', 'mix1'],\n    'runs': 10, # 10\n    'epochs': 10, # 10\n    'train_batch_size': 32,\n    'dev_batch_size': 32,\n    'test_batch_size': 32,\n    'label_list': ['O', 'B', 'I'],\n    'model_checkpoint': 'bert-base-uncased',\n    'lr': 1e-4, \n    'T': 3.0, # temperature for dwa weighting\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:40:36.940292Z","iopub.execute_input":"2023-08-28T16:40:36.940921Z","iopub.status.idle":"2023-08-28T16:40:36.950463Z","shell.execute_reply.started":"2023-08-28T16:40:36.940878Z","shell.execute_reply":"2023-08-28T16:40:36.947410Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\"\"\" Tokenize examples in batch\nSince the tokenizer may divide each token into two or more subtokens, we must align the new tokens with the original labels.\nNew subtokens must have the same label than their parent token\nLabels may be 0, 1 or 2 for O, B and I labels, respectively, and -100 for complementary tokens, such PAD, SEP, CLS tokens.\nLoss functions will ignore labels with value -100, so the loss only considers mistakes at the positions of real input (sub)tokens.\n\"\"\"\ndef tokenize_and_align_labels(txts, lbls, tokenizer, max_len = 128, mapping = None):\n\n    tokenized_inputs = tokenizer(txts, is_split_into_words=True,\n                                 max_length = max_len, padding = 'max_length', truncation=True,\n                                 return_tensors = 'pt')\n\n    labels = []\n    for i, label in enumerate(lbls):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        previous_label = None\n        label_ids = []\n        for word_idx in word_ids:\n            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n            # ignored in the loss function.\n            if word_idx is None:\n                label_ids.append('O')\n            # We set the label for the first token of each word.\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n                previous_label = label[word_idx]\n            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n            # the label_all_tokens flag.\n            else:\n                new_label = 'O'\n                if previous_label == 'O':\n                    new_label = 'O'\n                    # label_ids.append('O')\n                else:\n                    suffix_label = label[word_idx][1:]\n                    new_label = 'I'+suffix_label\n                label_ids.append(new_label)\n                previous_label = new_label\n\n            previous_word_idx = word_idx\n\n        labels.append(label_ids)\n\n    if mapping is not None:\n        labels = [list(map(lambda x : mapping.index(x), x)) for x in labels]\n\n    return tokenized_inputs, labels\n\n\"\"\"\nReturn text tokens from the tokenizer given the numeric input ids\n\"\"\"\ndef get_tokens_from_ids(input_ids):\n\n    return [tokenizer.convert_ids_to_tokens(tl) for tl in input_ids]\n\n\"\"\"\nRemove part of predicted sequences corresponding to padding tokens.\n\"\"\"\ndef remove_padding_from_predictions(predictions, batch_attention_mask):\n    valid_predictions_list = []\n    for instance_preds, att_mask in zip(predictions, batch_attention_mask):\n        valid = [pred for pred, mask in zip(instance_preds, att_mask) if mask == 1]\n        valid_predictions_list.append(valid[1:-1])\n        \n    return valid_predictions_list\n\ndef remove_padding_and_get_tokens(batch_ids, batch_attention_mask):\n    valid_ids_list = []\n    for instances_ids, att_mask in zip(batch_ids, batch_attention_mask):\n        valid = [ids for ids, mask in zip(instances_ids, att_mask) if mask == 1]\n        valid_ids_list.append(valid[1:-1])\n    \n    valid_tokens = get_tokens_from_ids(valid_ids_list)\n    return valid_tokens\n\n\"\"\"\nMaps sequences of integer to sequences of BIO tags\n\"\"\"\ndef integer_to_bio(labels, mapping):\n    return [[mapping[int(x)] for x in l] for l in labels]\n\n\"\"\"\nTransforms list of predicted sequences to a flat list of labels.\n\"\"\"\ndef flatten_predictions(labels):\n    return [j for sub in labels for j in sub]\n\n\"\"\"\nGenerates txt file with tokens, labels and predictions. Estilo FLAiR NLP.\n\"\"\"\ndef generate_results_txt(tokens, labels, predictions, output_file_name):\n    \n    with open(output_file_name, 'w', encoding = 'utf-8') as nf:\n\n        for tks, lbs, prds in zip(tokens, labels, predictions):\n            for tk, lb, pr in zip(tks, lbs, prds):\n                nf.write(f\"{tk} {lb} {pr}\\n\")\n\n            nf.write(f\"\\n\")\n\n\"\"\"\nDataset class for sequence labeling\n\"\"\"\nclass SequenceLabelingDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer, label_list):\n        MAX_LEN = 128\n        lb = [x.split() for x in df.labels.values.tolist()]\n        txt = [i.split() for i in df.tokens.values.tolist()]\n        self.encodings, self.labels = tokenize_and_align_labels(txt,\n                                                                lb,\n                                                                tokenizer,\n                                                                max_len = MAX_LEN,\n                                                                mapping = label_list)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n    \nclass MultitaskDataLoader(torch.utils.data.DataLoader):\n\n    def __init__(self, dataloaders):\n        self.dataloaders = dataloaders\n        self.min_length = min([len(d) for d in dataloaders])\n        self.lengths = [min(len(d), self.min_length) for d in dataloaders]\n        self.iterators = None\n        indices = [[i] * v for i, v in enumerate(self.lengths)]\n        self.task_indices = sum(indices, [])\n\n    def _reset(self):\n        random.shuffle(self.task_indices)\n        self.current_index = 0\n\n    def __iter__(self):\n        self._reset()\n        self.iterators = [iter(d) for d in self.dataloaders]\n        return self\n\n    def __len__(self):\n        return sum(self.lengths)\n\n    def __next__(self):\n        if self.current_index < len(self.task_indices):\n            task_index = self.task_indices[self.current_index]\n            batch = next(self.iterators[task_index])\n            self.current_index += 1\n            return batch, task_index\n        else:\n            raise StopIteration\n            \ndef initialize_crf_layer(layer):\n    for name, param in layer.named_parameters():\n        if \"transitions\" in name:\n            # Initialize transitions matrix with distinct values\n            init.uniform_(param, a=0.5, b=1.5)\n    \n    \nclass MultiTaskBERTCRFModel(nn.Module):\n    def __init__(self, model_checkpoint, num_labels = 3):\n        super(MultiTaskBERTCRFModel, self).__init__()\n        self.model_checkpoint = model_checkpoint\n        self.num_labels = num_labels\n        \n        self.transf = AutoModelForTokenClassification.from_pretrained(\n            self.model_checkpoint, \n            num_labels = self.num_labels\n        )\n        \n        self.crf_1 = CRF(self.num_labels, batch_first=True)\n        self.crf_2 = CRF(self.num_labels, batch_first=True)\n        self.crf_3 = CRF(self.num_labels, batch_first=True)\n        \n        # Initialize each CRF layer using the custom method\n        initialize_crf_layer(self.crf_1)\n        initialize_crf_layer(self.crf_2)\n        initialize_crf_layer(self.crf_3)\n\n    def forward(self, input_ids, attention_mask, token_type_ids = None, labels = None, task_id = 0):\n        task_id = 0 if task_id > 2 else task_id\n        \n        if token_type_ids is not None:\n            outputs = self.transf(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n        else:\n            outputs = self.transf(input_ids = input_ids, attention_mask = attention_mask)\n            \n        logits = outputs.logits\n        \n        if task_id == 0:\n            if labels is not None:\n                loss = -self.crf_1(logits, labels, mask=attention_mask.byte(), reduction = 'token_mean')\n                decoding = self.crf_1.decode(logits)\n                return loss, decoding\n            else:\n                return self.crf_1.decode(logits)\n        elif task_id == 1:\n            if labels is not None:\n                loss = -self.crf_2(logits, labels, mask=attention_mask.byte(), reduction = 'token_mean')\n                decoding = self.crf_2.decode(logits)\n                return loss, decoding\n            else:\n                return self.crf_2.decode(logits)\n        elif task_id == 2:\n            if labels is not None:\n                loss = -self.crf_3(logits, labels, mask=attention_mask.byte(), reduction = 'token_mean')\n                decoding = self.crf_3.decode(logits)\n                return loss, decoding\n            else:\n                return self.crf_3.decode(logits)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:40:39.084775Z","iopub.execute_input":"2023-08-28T16:40:39.085146Z","iopub.status.idle":"2023-08-28T16:40:39.124787Z","shell.execute_reply.started":"2023-08-28T16:40:39.085115Z","shell.execute_reply":"2023-08-28T16:40:39.123412Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def load_training_data(tasks, tokenizer, TRAIN_BATCH_SIZE = 32, DEV_BATCH_SIZE = 32):\n    \n    dataloaders = []\n    \n    min_train_instances = float(\"inf\")\n    min_dev_instances = float(\"inf\")\n    \n    train_dfs, dev_dfs = [], []\n    \n    for df_name in tasks:\n        df = pd.read_csv(f'/kaggle/input/unit-segmentation-lstm-transformers/{df_name}.csv')\n        train_seq_df, dev_seq_df = None, None\n        if 'dev' in df.set.unique():\n            train_seq_df = df.loc[df['set'] == 'train']\n            dev_seq_df = df.loc[df['set'] == 'dev']\n        else:\n            train_seq_df = df.loc[df['set'] == 'train']\n            train_seq_df, dev_seq_df = train_test_split(train_seq_df, test_size = 0.1, random_state = 2023)\n            \n        train_dfs.append(train_seq_df)\n        dev_dfs.append(dev_seq_df)\n            \n        if len(train_seq_df) < min_train_instances:\n            min_train_instances = len(train_seq_df)\n        if len(dev_seq_df) < min_dev_instances:\n            min_dev_instances = len(dev_seq_df)\n            \n    for train_seq_df, dev_seq_df in zip(train_dfs, dev_dfs):\n        train_seq_df = train_seq_df.sample(n = min_train_instances)\n        dev_seq_df = dev_seq_df.sample(n = min_dev_instances)\n        print(train_seq_df.shape, dev_seq_df.shape)\n        \n        # PYTORCH DATASETS\n        train_dataset = SequenceLabelingDataset(train_seq_df, tokenizer, label_list)\n        val_dataset = SequenceLabelingDataset(dev_seq_df, tokenizer, label_list)\n\n        # DATALOADERS\n        train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=DEV_BATCH_SIZE, shuffle=True)\n        \n        dataloaders.append((train_loader, val_loader))\n        \n    return dataloaders\n\n\n# def load_training_data(tasks, tokenizer, TRAIN_BATCH_SIZE = 32, DEV_BATCH_SIZE = 32):\n    \n#     dataloaders = []\n#     min_train_instances = float(\"inf\")\n#     min_dev_instances = float(\"inf\")\n    \n#     for df_name in tasks:\n#         df = pd.read_csv(f'/kaggle/input/unit-segmentation-lstm-transformers/{df_name}.csv')\n        \n#         train_seq_df = df.loc[df['set'] == 'train']\n#         train_seq_df, dev_seq_df = train_test_split(train_seq_df, test_size = 0.1, random_state = 2023)\n        \n#         if len(train_seq_df) < min_train_instances:\n#             min_train_instances = len(train_seq_df)\n#         if len(dev_seq_df) < min_dev_instances:\n#             min_dev_instances = len(dev_seq_df)\n                \n#     for i, df_name in enumerate(tasks, start = 1):\n    \n#         # SEQUENCE LABELING DATASET\n#         df = pd.read_csv(f'/kaggle/input/unit-segmentation-lstm-transformers/{df_name}.csv')\n\n#         train_seq_df, dev_seq_df = None, None\n\n#         if 'dev' in df.set.unique():\n#             train_seq_df = df.loc[df['set'] == 'train']\n#             dev_seq_df = df.loc[df['set'] == 'dev']\n#         else:\n#             train_seq_df = df.loc[df['set'] == 'train']\n#             train_seq_df, dev_seq_df = train_test_split(train_seq_df, test_size = 0.1, random_state = 2023)\n        \n#         train_seq_df = train_seq_df.sample(n = min_train_instances)\n#         dev_seq_df = dev_seq_df.sample(n = min_dev_instances)\n\n#         print(train_seq_df.shape, dev_seq_df.shape)\n    \n#         # PYTORCH DATASETS\n#         train_dataset = SequenceLabelingDataset(train_seq_df, tokenizer, label_list)\n#         val_dataset = SequenceLabelingDataset(dev_seq_df, tokenizer, label_list)\n\n#         # DATALOADERS\n#         train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n#         val_loader = DataLoader(val_dataset, batch_size=DEV_BATCH_SIZE, shuffle=True)\n        \n#         dataloaders.append((train_loader, val_loader))\n    \n#     return dataloaders\n    \ndef load_testing_data(df_name, tokenizer, TEST_BATCH_SIZE = 32):\n\n    # SEQUENCE LABELING DATASET\n    df = pd.read_csv(f'/kaggle/input/unit-segmentation-lstm-transformers/{df_name}.csv')\n    \n    test_seq_df = df.loc[df['set'] == 'test']\n    \n    # PYTORCH DATASETS\n    test_dataset = SequenceLabelingDataset(test_seq_df, tokenizer, label_list)\n    \n    # DATALOADERS\n    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n    \n    return test_loader","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:40:40.312254Z","iopub.execute_input":"2023-08-28T16:40:40.312790Z","iopub.status.idle":"2023-08-28T16:40:40.326217Z","shell.execute_reply.started":"2023-08-28T16:40:40.312755Z","shell.execute_reply":"2023-08-28T16:40:40.325102Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, optimizer, lambda_weight, epoch):\n\n    model.train()\n\n    train_loss = 0\n    cost_list = np.zeros(3, dtype=np.float32) # ver 3\n    num_batches_per_task = np.zeros(3, dtype=np.int32) # ver 3\n\n    for batch, task_id in train_loader:\n        \n        batch = tuple(v.to(device) for t, v in batch.items())\n        loss, outputs = None, None\n        \n        if model_checkpoint.startswith('bert'):\n            batch_input_ids, batch_token_type_ids, batch_attention_mask, batch_labels = batch\n            loss, outputs = model(batch_input_ids, \n                                  token_type_ids = batch_token_type_ids,\n                                  attention_mask = batch_attention_mask, \n                                  labels = batch_labels,\n                                    task_id = task_id)\n        else:\n            batch_input_ids, batch_attention_mask, batch_labels = batch\n            loss, outputs = model(batch_input_ids, \n                                  attention_mask = batch_attention_mask, \n                                  labels = batch_labels, task_id = task_id)\n\n        \n        loss = lambda_weight[task_id, epoch] * loss\n\n        train_loss += loss.item()\n        \n        cost_list[task_id] += loss.item()\n        num_batches_per_task[task_id] += 1\n        \n        # backprop\n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n#         print(\"TASK\", task_id)\n        \n#         # Print gradients for CRF layers only\n#         for name, param in model.named_parameters():\n#             if 'crf' in name and param.grad is not None:\n#                 print(f\"Parameter {name}:\")\n#                 print(param.grad)\n#             else:\n#                 print(f\"Parameter {name} has no gradient\")\n        \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n\n    avg_train_loss = train_loss / len(train_loader)\n    avg_cost = [round((w/t), 5) for w, t in zip(cost_list, num_batches_per_task)]\n    return avg_train_loss, avg_cost\n\ndef evaluate_model(model, dataloader, lambda_weight, epoch):\n\n    model.eval()\n\n    eval_loss = 0\n    cost_list = np.zeros(3, dtype=np.float32)\n    num_batches_per_task = np.zeros(3, dtype=np.int32)\n    eval_labels, eval_predictions = [], []\n    \n    with torch.no_grad():\n        for batch, task_id in dataloader:\n            batch = tuple(v.to(device) for t, v in batch.items())\n            loss, outputs = None, None\n            \n            if model_checkpoint.startswith('bert'):\n                batch_input_ids, batch_token_type_ids, batch_attention_mask, batch_labels = batch\n                loss, outputs = model(batch_input_ids, \n                                      token_type_ids = batch_token_type_ids,\n                                      attention_mask = batch_attention_mask, \n                                      labels = batch_labels)\n            else:\n                batch_input_ids, batch_attention_mask, batch_labels = batch\n                loss, outputs = model(batch_input_ids, \n                                      attention_mask = batch_attention_mask, \n                                      labels = batch_labels)\n            \n            loss = lambda_weight[task_id, epoch] * loss\n            eval_loss += loss.item()\n            \n            cost_list[task_id] += loss.item()\n            num_batches_per_task[task_id] += 1\n            \n            valid_labels = remove_padding_from_predictions(batch_labels.detach().cpu().numpy(), batch_attention_mask.detach().cpu().numpy())\n            eval_labels += valid_labels\n            \n            valid_predictions = remove_padding_from_predictions(outputs, batch_attention_mask.detach().cpu().numpy())\n            eval_predictions += valid_predictions\n    \n    flattened_labels = flatten_predictions(eval_labels)\n    flattened_predictions = flatten_predictions(eval_predictions)\n    \n    eval_f1 = f1_score(flattened_labels, flattened_predictions, average = 'macro')\n    avg_cost = [round((w/t), 5) for w, t in zip(cost_list, num_batches_per_task)]\n    return eval_loss / len(dataloader), avg_cost, eval_f1\n\ndef test_model(model, dataloader, task_id, write_files = False, path_file = None):\n    \n    macro_f1_in = 0\n    \n    testing_info = []\n    \n    for crf_i in range(3):\n\n        model.eval()\n\n        eval_tokens, eval_labels, eval_predictions = [], [], []\n\n        with torch.no_grad():\n            for batch in dataloader:\n                batch = tuple(v.to(device) for t, v in batch.items())\n                loss, outputs = None, None\n\n                if model_checkpoint.startswith('bert'):\n                    batch_input_ids, batch_token_type_ids, batch_attention_mask, batch_labels = batch\n                    _, outputs = model(batch_input_ids, \n                                       token_type_ids = batch_token_type_ids,\n                                       attention_mask = batch_attention_mask, \n                                       labels = batch_labels,\n                                       task_id = crf_i)\n                else:\n                    batch_input_ids, batch_attention_mask, batch_labels = batch\n                    _, outputs = model(batch_input_ids, \n                                       attention_mask = batch_attention_mask, \n                                       labels = batch_labels,\n                                       task_id = crf_i)\n\n                valid_labels = remove_padding_from_predictions(batch_labels.detach().cpu().numpy(), batch_attention_mask.detach().cpu().numpy())\n                eval_labels += valid_labels\n\n                valid_predictions = remove_padding_from_predictions(outputs, batch_attention_mask.detach().cpu().numpy())\n                eval_predictions += valid_predictions\n\n                valid_tokens = remove_padding_and_get_tokens(batch_input_ids.detach().cpu().numpy(), \n                                                             batch_attention_mask.detach().cpu().numpy())\n                eval_tokens += valid_tokens\n            \n        flattened_labels = flatten_predictions(eval_labels)\n        flattened_predictions = flatten_predictions(eval_predictions)\n        report_info = classification_report(flattened_labels, flattened_predictions, target_names = label_list, output_dict = True)\n        accuracy, macro_f1 = report_info['accuracy'], report_info['macro avg']['f1-score']\n        o_f1, b_f1, i_f1 =  report_info['O']['f1-score'], report_info['B']['f1-score'], report_info['I']['f1-score']\n        testing_info.append((len(flattened_predictions), accuracy, macro_f1, o_f1, b_f1, i_f1, crf_i))\n\n        if crf_i == task_id:\n            macro_f1_in = macro_f1\n        \n        if write_files:\n            generate_results_txt(eval_tokens, eval_labels, eval_predictions, path_file + f'-CRF{crf_i}.txt')\n    \n    return testing_info, macro_f1_in","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:40:41.686497Z","iopub.execute_input":"2023-08-28T16:40:41.687677Z","iopub.status.idle":"2023-08-28T16:40:41.713798Z","shell.execute_reply.started":"2023-08-28T16:40:41.687634Z","shell.execute_reply":"2023-08-28T16:40:41.712874Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# MODEL CONFIGURATION\nmodel_checkpoint = configuration['model_checkpoint']\nmodel_name = model_checkpoint.split('-')[0]\n\nlabel_list = configuration['label_list']\nnum_labels = len(label_list)\n\n# TRAINING CONFIGURATION\nRUNS = configuration['runs']\nEPOCHS = configuration['epochs']\n\nTRAIN_BATCH_SIZE = configuration['train_batch_size']\nTEST_BATCH_SIZE = configuration['test_batch_size']\nDEV_BATCH_SIZE = configuration['dev_batch_size']\n\ntrain_tasks = configuration['train_tasks']\ntest_dfs = configuration['test']\n\nweighting_strategy = configuration['task_weighting']\nT = configuration['T'] # temperature for DWA\n\n# MISCELLANEOUS\nSAVE_INFORMATION = False\nSAVE_BEST_MODEL = False\n\n# TOKENIZER\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# ======================================== #\ntraining_info, testing_results, models_info = [], [], []\nstart_time = time.time()\ntasks_weights_in_training = []\n\nfor nrun in range(RUNS):\n    # Initialize\n    rs = generate_random_seed()\n    set_random_seed(rs)\n    \n    best_eval_loss = float('inf')\n    best_epoch = 0\n    best_model_state = None\n    \n    # Dataloaders\n    loaders = load_training_data(train_tasks, tokenizer, \n                                 TRAIN_BATCH_SIZE=TRAIN_BATCH_SIZE, \n                                 DEV_BATCH_SIZE=DEV_BATCH_SIZE)\n    \n    t1_train_loader, t1_dev_loader = loaders[0]\n    t2_train_loader, t2_dev_loader = loaders[1]\n    t3_train_loader, t3_dev_loader = loaders[2]\n    \n    print(len(t1_train_loader), len(t2_train_loader), len(t3_train_loader))\n    print(len(t1_dev_loader), len(t2_dev_loader), len(t3_dev_loader))\n\n    combined_dataloader = MultitaskDataLoader([t1_train_loader, t2_train_loader, t3_train_loader])\n    combined_dev_dataloader = MultitaskDataLoader([t1_dev_loader, t2_dev_loader, t3_dev_loader])\n    \n    print(len(combined_dataloader), len(combined_dev_dataloader))\n        \n    # Create model\n    tagger = MultiTaskBERTCRFModel(model_checkpoint, num_labels = num_labels)\n    tagger.to(device)\n    \n    # OPTIMIZER\n    optimizer = torch.optim.AdamW(tagger.parameters(), lr = configuration['lr'], eps = 1e-8)\n    \n    avg_cost = np.zeros([EPOCHS, 3], dtype=np.float32) # ver 3\n    lambda_weight = np.ones([3, EPOCHS])\n\n    # Training loop\n    for index in range(EPOCHS):\n        print(f\"{index+1}/{EPOCHS}\")\n                \n        if weighting_strategy == 'dwa':\n            if index == 0 or index == 1:\n                lambda_weight[:, index] = 1.0\n            else:\n                w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n                w_2 = avg_cost[index - 1, 1] / avg_cost[index - 2, 1]\n                w_3 = avg_cost[index - 1, 2] / avg_cost[index - 2, 2]\n                lambda_weight[0, index] = 3 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n                lambda_weight[1, index] = 3 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n                lambda_weight[2, index] = 3 * np.exp(w_3 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n                \n        elif weighting_strategy == 'equal':\n            lambda_weight[:, index] = 1.0\n        \n        train_loss, train_costs = train_model(tagger, combined_dataloader, optimizer, lambda_weight, index)\n        avg_cost[index, :] = train_costs\n                \n        # evaluate model\n        eval_loss, eval_costs, eval_f1 = evaluate_model(tagger, combined_dev_dataloader, lambda_weight, index)\n        \n        training_info.append((nrun, rs, weighting_strategy, index, train_loss, eval_loss, eval_f1)) # nrun, epoch, train_loss, eval_loss, eval_f1\n        \n        # save best model based on validation loss\n        if eval_loss < best_eval_loss:\n            best_eval_loss = eval_loss\n            best_epoch = index\n            best_model_state = copy.deepcopy(tagger.state_dict())\n            \n        print(\"Train loss:\", train_loss, \" - Eval loss:\", eval_loss)\n            \n    print(f\"Best epoch: {best_epoch} - Validation loss: {best_eval_loss} [Run: {nrun}]\")\n        \n    avg_costs_dataframe = pd.DataFrame(avg_cost, columns = train_tasks)\n    avg_costs_dataframe['epoch'] = [i for i in range(EPOCHS)]\n    avg_costs_dataframe['run'] = nrun\n    avg_costs_dataframe.to_csv(f'avgcosts-mtl-{weighting_strategy}-{model_name}-{nrun}.csv', index = False)\n    lambda_weights_dataframe = pd.DataFrame(lambda_weight, columns = [f'epoch_{i}' for i in range(EPOCHS)])\n    lambda_weights_dataframe['task'] = train_tasks\n    lambda_weights_dataframe['run'] = nrun\n    lambda_weights_dataframe.to_csv(f'weigths-mtl-{weighting_strategy}-{model_name}-{nrun}.csv', index = False)\n    \n    # Testing\n    # Loading best model\n    best_tagger = None\n    best_tagger = MultiTaskBERTCRFModel(model_checkpoint, num_labels = num_labels)\n    best_tagger.load_state_dict(best_model_state)\n    best_tagger.to(device)\n    eval_results = evaluate_model(best_tagger, combined_dev_dataloader, lambda_weight, best_epoch)\n    print(eval_results) # check model\n    \n    macros = []\n        \n    for test_i, test_df in enumerate(test_dfs[:3]):\n        test_loader = load_testing_data(test_df, tokenizer, TEST_BATCH_SIZE = TEST_BATCH_SIZE)\n        path_test_results_file = f'results-mtl-{weighting_strategy}-{test_df}-{model_name}-{nrun}'\n        \n        testing_info, macro_f1_in = test_model(best_tagger, test_loader, test_i, write_files = True, path_file = path_test_results_file)\n        macros.append(macro_f1_in)\n        testing_info = [[nrun, test_df, weighting_strategy] + list(l) for l in testing_info]\n        testing_results += testing_info\n        \n    print(f\"Test Macros F1: {test_dfs[:3]}: {macros} [Run: {nrun}]\")\n    \n    if SAVE_BEST_MODEL:\n        model_path = f\"model-mtl-{weighting_strategy}-{model_name}-{nrun}.pt\"\n        models_info.append((nrun, model_path))\n        if best_model_state is not None:\n            torch.save(best_model_state, model_path)\n        else:\n            torch.save(tagger.state_dict(), model_path)\n    \n# Save data\nif SAVE_INFORMATION:\n    models_file_name = f\"models-mtl-{weighting_strategy}-{model_name}.csv\"\n    pd.DataFrame(models_info, columns = ['run', 'model_file']).to_csv(models_file_name, index = False)\n\n    train_file_name = f'train-info-mtl-{weighting_strategy}-{model_name}.csv'\n    pd.DataFrame(training_info, columns = ['run', 'seed', 'weighting', 'epoch', 'train_loss', 'eval_loss', 'eval_f1']).to_csv(train_file_name, index = False)\n\n    test_file_name = f'test-info-mtl-{weighting_strategy}-{model_name}.csv'\n    pd.DataFrame(testing_results, columns = ['run', 'test', 'weighting', 'sequences', 'accuracy', 'macro-f1', 'O-f1', 'B-f1', 'I-f1', 'crf_number']).to_csv(test_file_name, index = False)\n\nprint(f\"Total time: {((time.time() - start_time)//60)+1} minutes.\")","metadata":{"execution":{"iopub.status.busy":"2023-08-28T16:40:44.948606Z","iopub.execute_input":"2023-08-28T16:40:44.948977Z","iopub.status.idle":"2023-08-28T21:50:11.387563Z","shell.execute_reply.started":"2023-08-28T16:40:44.948940Z","shell.execute_reply":"2023-08-28T21:50:11.386425Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"261f4ed50f934600a533e64cd54a8338"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d2e313a35154a1c95b97fde72cabd7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11b13489257d4cba851cce1fec8cfdec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1b2f6015df4991970089a8d38fe314"}},"metadata":{}},{"name":"stdout","text":"(3170, 4) (353, 4)\n(3170, 4) (353, 4)\n(3170, 5) (353, 5)\n100 100 100\n12 12 12\n300 36\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0515e386e0f64343a87c8b6be1997213"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1/10\nTrain loss: 0.44550098677476246  - Eval loss: 0.30690746899280286\n2/10\nTrain loss: 0.2806907516469558  - Eval loss: 0.3860560388291358\n3/10\nTrain loss: 0.16045961274765433  - Eval loss: 0.44689356561543214\n4/10\nTrain loss: 0.09400857888860628  - Eval loss: 0.5031002562532699\n5/10\nTrain loss: 0.06445036849996541  - Eval loss: 0.6319211005078008\n6/10\nTrain loss: 0.05173601178297152  - Eval loss: 0.6103593868526028\n7/10\nTrain loss: 0.03513491220030119  - Eval loss: 0.6727559263688616\n8/10\nTrain loss: 0.03450004195192984  - Eval loss: 0.6918535090864881\n9/10\nTrain loss: 0.026216724421828985  - Eval loss: 0.7900057955654726\n10/10\nTrain loss: 0.02321261842303405  - Eval loss: 0.7264539451377156\nBest epoch: 0 - Validation loss: 0.30690746899280286 [Run: 0]\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"(0.3077634213388794, [0.34187, 0.1713, 0.41013], 0.7769783553714902)\nTest Macros F1: ['pe', 'we', 'abam']: [0.8422613134611119, 0.7702394298907072, 0.7435949583191984] [Run: 0]\n(3170, 4) (353, 4)\n(3170, 4) (353, 4)\n(3170, 5) (353, 5)\n100 100 100\n12 12 12\n300 36\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1/10\nTrain loss: 0.3126148347556591  - Eval loss: 0.22415619767788383\n2/10\nTrain loss: 0.1909985652503868  - Eval loss: 0.2249113597596685\n3/10\nTrain loss: 0.11216709988812605  - Eval loss: 0.4574094511982467\n4/10\nTrain loss: 0.06454714872253438  - Eval loss: 0.4774934549867693\n5/10\nTrain loss: 0.04678307459418041  - Eval loss: 0.36700395799966323\n6/10\nTrain loss: 0.03520233413573199  - Eval loss: 0.3917095427799116\n7/10\nTrain loss: 0.026660833528536994  - Eval loss: 0.45284169888408443\n8/10\nTrain loss: 0.021338330280632364  - Eval loss: 0.4420227291516817\n9/10\nTrain loss: 0.019200407678241996  - Eval loss: 0.5053682688281292\n10/10\nTrain loss: 0.017358188151993092  - Eval loss: 0.535389780045888\nBest epoch: 0 - Validation loss: 0.22415619767788383 [Run: 1]\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"(0.2368345713799095, [0.31361, 0.09698, 0.29991], 0.7921509286323133)\nTest Macros F1: ['pe', 'we', 'abam']: [0.8080804747562039, 0.8339199059540054, 0.7421149812247871] [Run: 1]\n(3170, 4) (353, 4)\n(3170, 4) (353, 4)\n(3170, 5) (353, 5)\n100 100 100\n12 12 12\n300 36\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1/10\nTrain loss: 0.42023673601448536  - Eval loss: 0.3014395504982935\n2/10\nTrain loss: 0.2621694042844077  - Eval loss: 0.29523990743069184\n3/10\nTrain loss: 0.15384310921809324  - Eval loss: 0.32482142829879496\n4/10\nTrain loss: 0.08928708138875663  - Eval loss: 0.48083058984613875\n5/10\nTrain loss: 0.06992537755907202  - Eval loss: 0.5901630664011464\n6/10\nTrain loss: 0.04657700938648001  - Eval loss: 0.5379894433984495\n7/10\nTrain loss: 0.04093290744504581  - Eval loss: 0.6246826580285819\n8/10\nTrain loss: 0.031211521718457032  - Eval loss: 0.6891202016397275\n9/10\nTrain loss: 0.03394281615037471  - Eval loss: 0.700527346563629\n10/10\nTrain loss: 0.029198422427337695  - Eval loss: 0.7729037159006111\nBest epoch: 1 - Validation loss: 0.29523990743069184 [Run: 2]\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"(0.3493618124889003, [0.53711, 0.14184, 0.36914], 0.8150114982325025)\nTest Macros F1: ['pe', 'we', 'abam']: [0.8278354460211319, 0.8185976667282882, 0.7596384741669647] [Run: 2]\n(3170, 4) (353, 4)\n(3170, 4) (353, 4)\n(3170, 5) (353, 5)\n100 100 100\n12 12 12\n300 36\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1/10\nTrain loss: 0.3501421345522006  - Eval loss: 0.24860207133719492\n2/10\nTrain loss: 0.21771106708794832  - Eval loss: 0.28005715273320675\n3/10\nTrain loss: 0.13145439279576143  - Eval loss: 0.289727309692858\n4/10\nTrain loss: 0.07796900941214213  - Eval loss: 0.3644715774312822\n5/10\nTrain loss: 0.04820972866378725  - Eval loss: 0.3799394837211973\n6/10\nTrain loss: 0.03857670333003625  - Eval loss: 0.38838657189787934\n7/10\nTrain loss: 0.03372365671636847  - Eval loss: 0.5592354876070103\n8/10\nTrain loss: 0.026279408061139597  - Eval loss: 0.5082901803139571\n9/10\nTrain loss: 0.02351268137562632  - Eval loss: 0.593562066115232\n10/10\nTrain loss: 0.020355944226127274  - Eval loss: 0.581841959950705\nBest epoch: 0 - Validation loss: 0.24860207133719492 [Run: 3]\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"(0.2721691444102261, [0.31298, 0.12573, 0.3778], 0.8113280531227373)\nTest Macros F1: ['pe', 'we', 'abam']: [0.8506694511073448, 0.8020140960627321, 0.7648885492863386] [Run: 3]\n(3170, 4) (353, 4)\n(3170, 4) (353, 4)\n(3170, 5) (353, 5)\n100 100 100\n12 12 12\n300 36\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1/10\nTrain loss: 0.3584136017970741  - Eval loss: 0.38599679836382467\n2/10\nTrain loss: 0.21994977737466495  - Eval loss: 0.2919697820778108\n3/10\nTrain loss: 0.13223541631673774  - Eval loss: 0.34949722617036766\n4/10\nTrain loss: 0.07763396651561683  - Eval loss: 0.4526403410264821\n5/10\nTrain loss: 0.05129751736142983  - Eval loss: 0.4831337952689662\n6/10\nTrain loss: 0.03556253355287481  - Eval loss: 0.5117092313755873\n7/10\nTrain loss: 0.030704230119202595  - Eval loss: 0.520859638645359\n8/10\nTrain loss: 0.026979097493070488  - Eval loss: 0.6313486301928101\n9/10\nTrain loss: 0.02181126799934039  - Eval loss: 0.7139804110699212\n10/10\nTrain loss: 0.017753180345898727  - Eval loss: 0.5914129651488716\nBest epoch: 1 - Validation loss: 0.2919697820778108 [Run: 4]\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"(0.2842998511882292, [0.35442, 0.11825, 0.38023], 0.8012670897786336)\nTest Macros F1: ['pe', 'we', 'abam']: [0.8402657884720225, 0.8008840707041269, 0.7059588656607613] [Run: 4]\n(3170, 4) (353, 4)\n(3170, 4) (353, 4)\n(3170, 5) (353, 5)\n100 100 100\n12 12 12\n300 36\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1/10\nTrain loss: 0.4126442242413759  - Eval loss: 0.26999193740387756\n2/10\nTrain loss: 0.2403358727445205  - Eval loss: 0.28232528150288594\n3/10\nTrain loss: 0.13568408231871823  - Eval loss: 0.3332904183642111\n4/10\nTrain loss: 0.08340209594442664  - Eval loss: 0.4483463304422912\n5/10\nTrain loss: 0.055148831298574806  - Eval loss: 0.5385327821333905\n6/10\nTrain loss: 0.03973786185165712  - Eval loss: 0.5714738277407984\n7/10\nTrain loss: 0.035112587004162685  - Eval loss: 0.5215531334365046\n8/10\nTrain loss: 0.028750015837722458  - Eval loss: 0.4804880033956983\n9/10\nTrain loss: 0.02442344332506764  - Eval loss: 0.6234064837110256\n10/10\nTrain loss: 0.02301453563491426  - Eval loss: 0.6787409901606023\nBest epoch: 0 - Validation loss: 0.26999193740387756 [Run: 5]\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"(0.2833687048405409, [0.34157, 0.17038, 0.33816], 0.7844063571698139)\nTest Macros F1: ['pe', 'we', 'abam']: [0.8456047363174249, 0.7490021245562289, 0.7387697127109744] [Run: 5]\n(3170, 4) (353, 4)\n(3170, 4) (353, 4)\n(3170, 5) (353, 5)\n100 100 100\n12 12 12\n300 36\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1/10\nTrain loss: 0.39561032368491095  - Eval loss: 0.4180910562475522\n2/10\nTrain loss: 0.23978991178795694  - Eval loss: 0.35811405152910286\n3/10\nTrain loss: 0.13343911397581298  - Eval loss: 0.6073508027734028\n4/10\nTrain loss: 0.08411357692598055  - Eval loss: 0.46293182068297434\n5/10\nTrain loss: 0.057800272167660295  - Eval loss: 0.49612084962023395\n6/10\nTrain loss: 0.04407473668882934  - Eval loss: 0.5103008135014938\n7/10\nTrain loss: 0.033699138983502054  - Eval loss: 0.6065571972802799\n8/10\nTrain loss: 0.03328276328276843  - Eval loss: 0.6152946677401714\n9/10\nTrain loss: 0.024222516080966063  - Eval loss: 0.7477710435632616\n10/10\nTrain loss: 0.025116460831074317  - Eval loss: 0.6937936562212094\nBest epoch: 1 - Validation loss: 0.35811405152910286 [Run: 6]\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"(0.3138275498317348, [0.37352, 0.14986, 0.4181], 0.7988352390598198)\nTest Macros F1: ['pe', 'we', 'abam']: [0.8198051795939513, 0.7836124743884357, 0.7421307257596453] [Run: 6]\n(3170, 4) (353, 4)\n(3170, 4) (353, 4)\n(3170, 5) (353, 5)\n100 100 100\n12 12 12\n300 36\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1/10\nTrain loss: 0.4203545810189098  - Eval loss: 0.6169564652567109\n2/10\nTrain loss: 0.24906374450773  - Eval loss: 0.331402131731415\n3/10\nTrain loss: 0.15008310082290943  - Eval loss: 0.35586127667273915\n4/10\nTrain loss: 0.08898429700483879  - Eval loss: 0.37865799298096037\n5/10\nTrain loss: 0.05940392278212433  - Eval loss: 0.45028563324982923\n6/10\nTrain loss: 0.040962743694350746  - Eval loss: 0.5612716351428794\n7/10\nTrain loss: 0.037578401261853286  - Eval loss: 0.47605513483964995\n8/10\nTrain loss: 0.0286256238851153  - Eval loss: 0.5431915904987868\n9/10\nTrain loss: 0.024485124061757232  - Eval loss: 0.5593932690822435\n10/10\nTrain loss: 0.019492080453637754  - Eval loss: 0.7818419095511773\nBest epoch: 1 - Validation loss: 0.331402131731415 [Run: 7]\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"(0.2651485903447287, [0.33923, 0.12155, 0.33466], 0.8012124087034955)\nTest Macros F1: ['pe', 'we', 'abam']: [0.8442200966481161, 0.7611305571773176, 0.7575510953926896] [Run: 7]\n(3170, 4) (353, 4)\n(3170, 4) (353, 4)\n(3170, 5) (353, 5)\n100 100 100\n12 12 12\n300 36\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1/10\nTrain loss: 0.3774295088897149  - Eval loss: 0.3393050142460399\n2/10\nTrain loss: 0.2393581862250964  - Eval loss: 0.33834828798555666\n3/10\nTrain loss: 0.13528249049559235  - Eval loss: 0.3408630297312306\n4/10\nTrain loss: 0.07906326825652893  - Eval loss: 0.48692731175429393\n5/10\nTrain loss: 0.05427361006693294  - Eval loss: 0.4854768298876782\n6/10\nTrain loss: 0.040277806699159556  - Eval loss: 0.6464886435343133\n7/10\nTrain loss: 0.03548189205709302  - Eval loss: 0.5739856914717367\n8/10\nTrain loss: 0.029478352855270108  - Eval loss: 0.6079479379792853\n9/10\nTrain loss: 0.028138346525180775  - Eval loss: 0.6537551864506289\n10/10\nTrain loss: 0.024350046224426478  - Eval loss: 0.6062682763594138\nBest epoch: 1 - Validation loss: 0.33834828798555666 [Run: 8]\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"(0.3235190527824064, [0.40255, 0.09898, 0.46902], 0.7707435426116559)\nTest Macros F1: ['pe', 'we', 'abam']: [0.8085872118126725, 0.8450052710175969, 0.6847134101597928] [Run: 8]\n(3170, 4) (353, 4)\n(3170, 4) (353, 4)\n(3170, 5) (353, 5)\n100 100 100\n12 12 12\n300 36\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1/10\nTrain loss: 0.42461799384405213  - Eval loss: 0.5602964444292916\n2/10\nTrain loss: 0.26658609326928856  - Eval loss: 0.4199461299253421\n3/10\nTrain loss: 0.15629082397557795  - Eval loss: 0.4649106189349873\n4/10\nTrain loss: 0.09544061477819923  - Eval loss: 0.5997527891563045\n5/10\nTrain loss: 0.06366249979007989  - Eval loss: 0.5229512815664444\n6/10\nTrain loss: 0.04813286109196876  - Eval loss: 0.5636011402166332\n7/10\nTrain loss: 0.04002485228552056  - Eval loss: 0.5906209253007546\n8/10\nTrain loss: 0.032284249031993874  - Eval loss: 0.6135444167173572\n9/10\nTrain loss: 0.02568569414220595  - Eval loss: 0.5857987442844509\n10/10\nTrain loss: 0.026567676110280446  - Eval loss: 0.6454030108822432\nBest epoch: 1 - Validation loss: 0.4199461299253421 [Run: 9]\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"(0.44076107292332584, [0.54711, 0.12304, 0.65214], 0.7843450690938529)\nTest Macros F1: ['pe', 'we', 'abam']: [0.8256970205277967, 0.8557962753536628, 0.7318342051312775] [Run: 9]\nTotal time: 310.0 minutes.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_file_name = f'train-info-mtl-{weighting_strategy}-{model_name}.csv'\npd.DataFrame(training_info, columns = ['run', 'seed', 'weighting', 'epoch', 'train_loss', 'eval_loss', 'eval_f1']).to_csv(train_file_name, index = False)\n\ntest_file_name = f'test-info-mtl-{weighting_strategy}-{model_name}.csv'\npd.DataFrame(testing_results, columns = ['run', 'test', 'weighting', 'sequences', 'accuracy', 'macro-f1', 'O-f1', 'B-f1', 'I-f1', 'crf_number']).to_csv(test_file_name, index = False)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T21:51:18.342333Z","iopub.execute_input":"2023-08-28T21:51:18.342713Z","iopub.status.idle":"2023-08-28T21:51:18.356564Z","shell.execute_reply.started":"2023-08-28T21:51:18.342682Z","shell.execute_reply":"2023-08-28T21:51:18.355630Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"avg_cost, lambda_weight","metadata":{"execution":{"iopub.status.busy":"2023-08-25T02:12:43.912020Z","iopub.execute_input":"2023-08-25T02:12:43.912593Z","iopub.status.idle":"2023-08-25T02:12:43.921024Z","shell.execute_reply.started":"2023-08-25T02:12:43.912532Z","shell.execute_reply":"2023-08-25T02:12:43.919844Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(array([[0.77332, 0.7129 , 1.13553],\n        [0.57892, 0.30982, 0.98344]], dtype=float32),\n array([[1., 1.],\n        [1., 1.],\n        [1., 1.]]))"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport zipfile\n\ndef zip_files(folder_path, zip_name):\n    # Crear un archivo ZIP\n    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Recorrer todos los archivos en la carpeta\n        for foldername, subfolders, filenames in os.walk(folder_path):\n            for filename in filenames:\n                # Comprobar si el archivo es un archivo TXT o CSV\n                if filename.endswith('.txt') or filename.endswith('.csv') or filename.endswith('.pt'):\n                    # Ruta completa del archivo\n                    file_path = os.path.join(foldername, filename)\n                    # Agregar el archivo al archivo ZIP\n                    zipf.write(file_path, os.path.relpath(file_path, folder_path))\n\n# Llamar a la función para comprimir los archivos\nfolder_path = '/kaggle/working/'\nzip_name = 'archivos2.zip'\nzip_files(folder_path, zip_name)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T21:51:20.967288Z","iopub.execute_input":"2023-08-28T21:51:20.968039Z","iopub.status.idle":"2023-08-28T21:51:22.734303Z","shell.execute_reply.started":"2023-08-28T21:51:20.968001Z","shell.execute_reply":"2023-08-28T21:51:22.733382Z"},"trusted":true},"execution_count":9,"outputs":[]}]}