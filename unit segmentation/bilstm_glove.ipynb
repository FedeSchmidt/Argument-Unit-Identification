{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3176,"sourceType":"datasetVersion","datasetId":1835},{"sourceId":6211788,"sourceType":"datasetVersion","datasetId":3304856}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\nimport json\nimport copy\nimport ast\nimport random\nimport time\nimport torch\nfrom torch import nn, cuda, optim\nfrom torch.utils.data import DataLoader\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nif device == 'cuda':\n    torch.cuda.empty_cache()\nprint(device)\n\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nstops = set(stopwords.words('english'))\n\ntry:\n    from torchcrf import CRF\nexcept:\n    !pip install pytorch-crf\n    from torchcrf import CRF\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    f1_score,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    precision_recall_fscore_support,\n    classification_report\n)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef generate_random_seed():\n    return random.randint(1, 1000)\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-06T17:44:33.302199Z","iopub.execute_input":"2023-08-06T17:44:33.302608Z","iopub.status.idle":"2023-08-06T17:44:51.473967Z","shell.execute_reply.started":"2023-08-06T17:44:33.302582Z","shell.execute_reply":"2023-08-06T17:44:51.472724Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Collecting pytorch-crf\n  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\nInstalling collected packages: pytorch-crf\nSuccessfully installed pytorch-crf-0.7.2\n/kaggle/input/unit-segmentation-lstm-transformers/we.csv\n/kaggle/input/unit-segmentation-lstm-transformers/mix1.csv\n/kaggle/input/unit-segmentation-lstm-transformers/pe.csv\n/kaggle/input/unit-segmentation-lstm-transformers/abam.csv\n/kaggle/input/unit-segmentation-lstm-transformers/mix2.csv\n/kaggle/input/unit-segmentation-lstm-transformers/ug.csv\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"# !wget http://nlp.stanford.edu/data/glove.6B.zip\n# !unzip -q glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-08-02T12:54:24.471110Z","iopub.execute_input":"2023-08-02T12:54:24.472742Z","iopub.status.idle":"2023-08-02T12:54:24.477751Z","shell.execute_reply.started":"2023-08-02T12:54:24.472692Z","shell.execute_reply":"2023-08-02T12:54:24.476677Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# get glove embeddings from file\nembeddings_index, embeddings_dim = {}, 200\nwith open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt') as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\nprint('Found {} word vectors.'.format(len(embeddings_index)))","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:45:37.282915Z","iopub.execute_input":"2023-08-06T17:45:37.284285Z","iopub.status.idle":"2023-08-06T17:46:03.810822Z","shell.execute_reply.started":"2023-08-06T17:45:37.284234Z","shell.execute_reply":"2023-08-06T17:46:03.809747Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 400000 word vectors.\n","output_type":"stream"}]},{"cell_type":"code","source":"configuration = {\n    'train': 'mix1', \n    'test': ['pe', 'we', 'abam', 'ug', 'mix1'],\n    'runs': 10,\n    'epochs': 20,\n    'train_batch_size': 32,\n    'dev_batch_size': 32,\n    'test_batch_size': 32,\n    'label_list': ['O', 'B', 'I'],\n    'model_checkpoint': 'glove',\n    'crf': True,\n    'lr': 1e-4, \n    'hidden_dim': 128\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:47:31.370272Z","iopub.execute_input":"2023-08-06T17:47:31.370646Z","iopub.status.idle":"2023-08-06T17:47:31.376689Z","shell.execute_reply.started":"2023-08-06T17:47:31.370617Z","shell.execute_reply":"2023-08-06T17:47:31.375780Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nClean sentence\n\"\"\"\ndef clean_sentence(text, remove_stop = False):\n    tokens = [token.strip().lower() for token in text.split() if not token in string.punctuation]\n    if remove_stop:\n        tokens = [token for token in tokens if not token in stops]\n    return \" \".join(tokens)\n\n\"\"\"\nRemove part of predicted sequences corresponding to padding tokens.\n\"\"\"\ndef remove_padding_from_predictions(predictions, batch_attention_mask):\n    valid_predictions_list = []\n    for instance_preds, att_mask in zip(predictions, batch_attention_mask):\n        valid = [pred for pred, mask in zip(instance_preds, att_mask) if mask == 1]\n        valid_predictions_list.append(valid)\n        \n    return valid_predictions_list\n\ndef remove_padding_and_get_tokens(batch_input_tokens, batch_attention_mask):\n    valid_tokens = []\n    for instances_ids, att_mask in zip(batch_input_tokens, batch_attention_mask):\n        valid = [ids for ids, mask in zip(instances_ids, att_mask) if mask == 1]\n        valid_tokens.append(valid)\n    \n    return valid_tokens\n\n\"\"\"\nMaps sequences of integer to sequences of BIO tags\n\"\"\"\ndef integer_to_bio(labels, mapping):\n    return [[mapping[int(x)] for x in l] for l in labels]\n\n\"\"\"\nTransforms list of predicted sequences to a flat list of labels.\n\"\"\"\ndef flatten_predictions(labels):\n    return [j for sub in labels for j in sub]\n\n\"\"\"\nGenerates txt file with tokens, labels and predictions. Estilo FLAiR NLP.\n\"\"\"\ndef generate_results_txt(tokens, labels, predictions, output_file_name):\n    \n    with open(output_file_name, 'w', encoding = 'utf-8') as nf:\n\n        for tks, lbs, prds in zip(tokens, labels, predictions):\n            for tk, lb, pr in zip(tks, lbs, prds):\n                nf.write(f\"{tk} {lb} {pr}\\n\")\n\n            nf.write(f\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:47:41.017202Z","iopub.execute_input":"2023-08-06T17:47:41.017578Z","iopub.status.idle":"2023-08-06T17:47:41.032622Z","shell.execute_reply.started":"2023-08-06T17:47:41.017545Z","shell.execute_reply":"2023-08-06T17:47:41.031039Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nDataset class for sequence labeling\n\"\"\"\nclass SequenceLabelingDataset(torch.utils.data.Dataset):\n    def __init__(self, df, label_list, glove_embeddings, embedding_dim = 200, max_sequence_length=128):\n        self.max_sequence_length = max_sequence_length\n        self.mapping = label_list\n        self.glove_embeddings = glove_embeddings\n        self.embedding_dim = embedding_dim\n        self.labels = [x.split() for x in df.labels.values.tolist()]\n        self.encodings = [i.split() for i in df.tokens.values.tolist()]\n\n    def __getitem__(self, idx):\n        sentence_tokens = self.encodings[idx]\n        sentence_labels = self.labels[idx]\n        sentence_labels = list(map(lambda x : self.mapping.index(x), sentence_labels))\n        \n        sentence_tokens = sentence_tokens[:self.max_sequence_length]\n        sentence_labels = sentence_labels[:self.max_sequence_length]\n        \n        padded_tokens = sentence_tokens + ['<PAD>'] * (self.max_sequence_length - len(sentence_tokens))\n        padded_labels = sentence_labels + [0] * (self.max_sequence_length - len(sentence_labels))\n        \n        # Encode tokens using GloVe embeddings\n        embeddings = torch.tensor([self.glove_embeddings.get(token, np.random.rand(self.embedding_dim).astype(np.float32)) for token in padded_tokens])\n        \n        # Create attention mask\n        attention_mask = torch.ones(self.max_sequence_length)\n        attention_mask[len(sentence_tokens):] = 0\n        \n#         print(padded_tokens)\n        \n        item = {'encodings' : embeddings,\n                'mask': attention_mask,\n                'tokens': padded_tokens,\n               'labels': torch.tensor(padded_labels)}\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n    \n\nclass BiLSTMSequenceLabelingModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, dropout, num_labels):\n        super(BiLSTMSequenceLabelingModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_labels = num_labels\n        self.lstm = nn.LSTM(input_dim, hidden_dim // 2, bidirectional=True, batch_first=True)\n        self.dropout = nn.Dropout(p=dropout)\n        self.hidden2tag = nn.Linear(hidden_dim, num_labels)\n        self.crf = CRF(self.num_labels, batch_first = True)\n\n    def forward(self, x, attention_mask, labels = None):\n        \n        lstm_out, _ = self.lstm(x)\n        \n        lstm_out = self.dropout(lstm_out)\n        \n        logits = self.hidden2tag(lstm_out)\n        \n        if labels is not None: # training\n            loss = -self.crf(logits, labels, mask = attention_mask.byte(), reduction = 'token_mean')\n            return loss, self.crf.decode(logits)\n        else: # inference\n            return self.crf.decode(logits)\n        \n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:48:14.831244Z","iopub.execute_input":"2023-08-06T17:48:14.831601Z","iopub.status.idle":"2023-08-06T17:48:14.848997Z","shell.execute_reply.started":"2023-08-06T17:48:14.831571Z","shell.execute_reply":"2023-08-06T17:48:14.848011Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def load_training_data(df_name, TRAIN_BATCH_SIZE = 32, DEV_BATCH_SIZE = 32):\n    \n    # SEQUENCE LABELING DATASET\n    df = pd.read_csv(f'/kaggle/input/unit-segmentation-lstm-transformers/{df_name}.csv')\n    \n#     df['tokens'] = df['tokens'].apply(lambda x: clean_sentence(x))\n    \n    train_seq_df, dev_seq_df = None, None\n    \n    if 'dev' in df.set.unique():\n        train_seq_df = df.loc[df['set'] == 'train']\n        dev_seq_df = df.loc[df['set'] == 'dev']\n    else:\n        train_seq_df = df.loc[df['set'] == 'train']\n        train_seq_df, dev_seq_df = train_test_split(train_seq_df, test_size = 0.1, random_state = 2023)\n        \n    train_seq_df = train_seq_df.sample(frac = 1)\n    dev_seq_df = dev_seq_df.sample(frac = 1)\n    \n    print(train_seq_df.shape, dev_seq_df.shape)\n    \n    # PYTORCH DATASETS\n    train_dataset = SequenceLabelingDataset(train_seq_df, label_list, embeddings_index)\n    val_dataset = SequenceLabelingDataset(dev_seq_df, label_list, embeddings_index)\n    \n    # DATALOADERS\n    train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=DEV_BATCH_SIZE, shuffle=True)\n    \n    return train_loader, val_loader\n    \ndef load_testing_data(df_name, TEST_BATCH_SIZE = 32):\n\n    # SEQUENCE LABELING DATASET\n    df = pd.read_csv(f'/kaggle/input/unit-segmentation-lstm-transformers/{df_name}.csv')\n    \n#     df['tokens'] = df['tokens'].apply(lambda x: clean_sentence(x))\n    \n    test_seq_df = df.loc[df['set'] == 'test']\n    \n    # PYTORCH DATASETS\n    test_dataset = SequenceLabelingDataset(test_seq_df, label_list, embeddings_index)\n    \n    # DATALOADERS\n    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n    \n    return test_loader","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:48:17.826046Z","iopub.execute_input":"2023-08-06T17:48:17.826405Z","iopub.status.idle":"2023-08-06T17:48:17.837799Z","shell.execute_reply.started":"2023-08-06T17:48:17.826377Z","shell.execute_reply":"2023-08-06T17:48:17.836643Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, optimizer):\n\n    model.train()\n\n    train_loss = 0\n    for batch in train_loader:\n        batch_inputs = batch['encodings'].to(device)\n        batch_attention_mask = batch['mask'].to(device)\n        batch_labels = batch['labels'].to(device)\n#         batch_tokens = batch['tokens']\n        \n        \n#         batch = tuple(v for t, v in batch.items())\n        loss, outputs = None, None\n                \n#         batch_inputs, batch_masks, batch_tokens, batch_labels = batch\n        loss, outputs = model(batch_inputs, batch_attention_mask, labels = batch_labels)\n\n        train_loss += loss.item()\n\n        # backprop\n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n\n    avg_train_loss = train_loss / len(train_loader)\n    return avg_train_loss\n\ndef evaluate_model(model, dataloader):\n\n    model.eval()\n\n    eval_loss = 0\n    eval_labels, eval_predictions = [], []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n#             batch = tuple(v.to(device) for t, v in batch.items())\n            loss, outputs = None, None\n            \n#             batch_inputs, batch_attention_mask, batch_tokens, batch_labels = batch\n            batch_inputs = batch['encodings'].to(device)\n            batch_attention_mask = batch['mask'].to(device)\n            batch_labels = batch['labels'].to(device)\n#             batch_tokens = batch['tokens']\n\n            loss, outputs = model(batch_inputs, batch_attention_mask, labels = batch_labels)\n            \n            eval_loss += loss.item()\n            \n            valid_labels = remove_padding_from_predictions(batch_labels.detach().cpu().numpy(), batch_attention_mask.detach().cpu().numpy())\n            eval_labels += valid_labels\n            \n            valid_predictions = remove_padding_from_predictions(outputs, batch_attention_mask.detach().cpu().numpy())\n            eval_predictions += valid_predictions\n    \n    flattened_labels = flatten_predictions(eval_labels)\n    flattened_predictions = flatten_predictions(eval_predictions)\n    \n    eval_f1 = f1_score(flattened_labels, flattened_predictions, average = 'macro')\n    return eval_loss / len(dataloader), eval_f1\n\n\ndef test_model(model, dataloader, write_file = False, path_file = None):\n\n    model.eval()\n\n    eval_tokens, eval_labels, eval_predictions = [], [], []\n\n    with torch.no_grad():\n        for batch in dataloader:\n#             batch = tuple(v.to(device) for t, v in batch.items())\n\n            loss, outputs = None, None\n            \n#             batch_inputs, batch_attention_mask, batch_tokens, batch_labels = batch\n            batch_inputs = batch['encodings'].to(device)\n            batch_attention_mask = batch['mask'].to(device)\n            batch_labels = batch['labels'].to(device)\n            batch_tokens = batch['tokens']\n\n            _, outputs = model(batch_inputs, batch_attention_mask, labels = batch_labels)\n\n            valid_labels = remove_padding_from_predictions(batch_labels.detach().cpu().numpy(), batch_attention_mask.detach().cpu().numpy())\n            eval_labels += valid_labels\n            \n            valid_predictions = remove_padding_from_predictions(outputs, batch_attention_mask.detach().cpu().numpy())\n            eval_predictions += valid_predictions\n            \n            valid_tokens = remove_padding_and_get_tokens(batch_tokens, \n                                                         batch_attention_mask.detach().cpu().numpy())\n            eval_tokens += valid_tokens\n            \n    if write_file:\n        generate_results_txt(eval_tokens, eval_labels, eval_predictions, path_file)\n    \n    return eval_labels, eval_predictions","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:48:24.355965Z","iopub.execute_input":"2023-08-06T17:48:24.356347Z","iopub.status.idle":"2023-08-06T17:48:24.374022Z","shell.execute_reply.started":"2023-08-06T17:48:24.356316Z","shell.execute_reply":"2023-08-06T17:48:24.373083Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# MODEL CONFIGURATION\nmodel_checkpoint = configuration['model_checkpoint']\nembedding_type = model_checkpoint.split('-')[0]\nlabel_list = configuration['label_list']\nnum_labels = len(label_list)\n\n# TRAINING CONFIGURATION\nRUNS = configuration['runs']\nEPOCHS = configuration['epochs']\nTRAIN_BATCH_SIZE = configuration['train_batch_size']\nTEST_BATCH_SIZE = configuration['test_batch_size']\nDEV_BATCH_SIZE = configuration['dev_batch_size']\ntrain_df = configuration['train']\ntest_dfs = configuration['test']\n\n# MISCELLANEOUS\nSAVE_INFORMATION = True\nSAVE_BEST_MODEL = True\n\n# ======================================== #\ntraining_info, testing_info, models_info = [], [], []\nstart_time = time.time()\nfor nrun in range(RUNS):\n    # Initialize\n    rs = generate_random_seed()\n    set_random_seed(rs)\n    \n    best_eval_loss = float('inf')\n    best_epoch = 0\n    best_model_state = None\n    \n    # Dataloaders\n    train_loader, val_loader = load_training_data(train_df, TRAIN_BATCH_SIZE=TRAIN_BATCH_SIZE, DEV_BATCH_SIZE=DEV_BATCH_SIZE)\n    \n    # Create model\n    tagger = BiLSTMSequenceLabelingModel(embeddings_dim, hidden_dim = configuration['hidden_dim'], dropout=0.2, num_labels = num_labels)\n    tagger.to(device)\n    \n    # OPTIMIZER\n    optimizer = torch.optim.AdamW(tagger.parameters(), lr = configuration['lr'], eps = 1e-8)\n    \n    # Training loop\n    for epoch in range(EPOCHS):\n        print(f\"{epoch+1}/{EPOCHS}\")\n        # train one epoch\n        train_loss = train_model(tagger, train_loader, optimizer)\n        \n        # evaluate model\n        eval_loss, eval_f1 = evaluate_model(tagger, val_loader)\n        \n        training_info.append((nrun, rs, epoch, train_loss, eval_loss, eval_f1)) # nrun, epoch, train_loss, eval_loss, eval_f1\n        \n        # save best model based on validation loss\n        if eval_loss < best_eval_loss:\n            best_eval_loss = eval_loss\n            best_epoch = epoch\n            best_model_state = copy.deepcopy(tagger.state_dict())\n            \n    print(f\"Best epoch: {best_epoch} - Validation loss: {best_eval_loss} [Run: {nrun}]\")\n    \n    # Testing\n    # Loading best model\n    best_tagger = BiLSTMSequenceLabelingModel(embeddings_dim, hidden_dim = configuration['hidden_dim'], dropout=0.2, num_labels = num_labels)    \n    best_tagger.load_state_dict(best_model_state)\n    best_tagger.to(device)\n    eval_results = evaluate_model(best_tagger, val_loader)\n    print(eval_results) # check model\n    \n    macros = []\n    for test_df in test_dfs:\n        test_loader = load_testing_data(test_df, TEST_BATCH_SIZE = TEST_BATCH_SIZE)\n        path_test_results_file = f'results-{train_df}-{test_df}-{embedding_type}-bilstmcrf-{nrun}.txt'\n        tlabels, tpredictions = test_model(best_tagger, test_loader, write_file = True, path_file = path_test_results_file)\n        \n        flattened_labels = flatten_predictions(tlabels)\n        flattened_predictions = flatten_predictions(tpredictions)\n        report_info = classification_report(flattened_labels, flattened_predictions, target_names = label_list, output_dict = True)\n        accuracy, macro_f1 = report_info['accuracy'], report_info['macro avg']['f1-score']\n        o_f1, b_f1, i_f1 =  report_info['O']['f1-score'], report_info['B']['f1-score'], report_info['I']['f1-score']\n        macros.append(macro_f1)\n    \n        testing_info.append((nrun, train_df, test_df, len(tpredictions), accuracy, macro_f1, o_f1, b_f1, i_f1)) # nrun, train, test, sequences, acc, macrof1, Of1, Bf1, If1\n    \n    print(f\"Test Macros F1: {test_dfs}: {macros} [Run: {nrun}]\")\n    \n    if SAVE_BEST_MODEL:\n        model_path = f\"model-{train_df}-{embedding_type}-bilstmcrf-{nrun}.pt\"\n        models_info.append((nrun, model_path))\n        if best_model_state is not None:\n            torch.save(best_model_state, model_path)\n        else:\n            torch.save(tagger.state_dict(), model_path)\n    \n# Save data    \nif SAVE_INFORMATION:\n    \n    models_file_name = f\"models-{train_df}-{embedding_type}-bilstmcrf.csv\"\n    pd.DataFrame(models_info, columns = ['run', 'model_file']).to_csv(models_file_name, index = False)\n    \n    train_file_name = f'train-info-{train_df}-{embedding_type}-bilstmcrf.csv'\n    pd.DataFrame(training_info, columns = ['run', 'seed', 'epoch', 'train_loss', 'eval_loss', 'eval_f1']).to_csv(train_file_name, index = False)\n    \n    test_file_name = f'test-info-{train_df}-{embedding_type}-bilstmcrf.csv'\n    pd.DataFrame(testing_info, columns = ['run', 'train', 'test', 'sequences', 'accuracy', 'macro-f1', 'O-f1', 'B-f1', 'I-f1']).to_csv(test_file_name, index = False)\n\nprint(f\"Total time: {((time.time() - start_time)//60)+1} minutes.\")","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:49:06.840925Z","iopub.execute_input":"2023-08-06T17:49:06.841779Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"(9512, 5) (1057, 5)\n1/20\n2/20\n3/20\n4/20\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport zipfile\n\ndef zip_files(folder_path, zip_name):\n    # Crear un archivo ZIP\n    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Recorrer todos los archivos en la carpeta\n        for foldername, subfolders, filenames in os.walk(folder_path):\n            for filename in filenames:\n                # Comprobar si el archivo es un archivo TXT o CSV\n                if filename.endswith('.txt') or filename.endswith('.csv') or filename.endswith('.pt'):\n                    # Ruta completa del archivo\n                    file_path = os.path.join(foldername, filename)\n                    # Agregar el archivo al archivo ZIP\n                    zipf.write(file_path, os.path.relpath(file_path, folder_path))\n\n# Llamar a la función para comprimir los archivos\nfolder_path = '/kaggle/working/'\nzip_name = 'archivos2.zip'\nzip_files(folder_path, zip_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}