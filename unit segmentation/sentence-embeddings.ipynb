{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6189082,"sourceType":"datasetVersion","datasetId":3304856}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\nimport torch\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nif device == 'cuda':\n    torch.cuda.empty_cache()\nprint(device)\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nstops = set(stopwords.words('english'))\n\nfrom transformers import AutoTokenizer, AutoModel\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-27T22:22:29.755418Z","iopub.execute_input":"2023-07-27T22:22:29.755818Z","iopub.status.idle":"2023-07-27T22:22:36.188352Z","shell.execute_reply.started":"2023-07-27T22:22:29.755784Z","shell.execute_reply":"2023-07-27T22:22:36.187064Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/unit-segmentation-lstm-transformers/we.csv\n/kaggle/input/unit-segmentation-lstm-transformers/mix1.csv\n/kaggle/input/unit-segmentation-lstm-transformers/pe.csv\n/kaggle/input/unit-segmentation-lstm-transformers/abam.csv\n/kaggle/input/unit-segmentation-lstm-transformers/mix2.csv\n/kaggle/input/unit-segmentation-lstm-transformers/ug.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Glove","metadata":{}},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip -q glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-07-27T22:22:50.819483Z","iopub.execute_input":"2023-07-27T22:22:50.820159Z","iopub.status.idle":"2023-07-27T22:25:56.716471Z","shell.execute_reply.started":"2023-07-27T22:22:50.820124Z","shell.execute_reply":"2023-07-27T22:25:56.715147Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2023-07-27 22:22:51--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2023-07-27 22:22:51--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2023-07-27 22:22:52--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: ‘glove.6B.zip’\n\nglove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n\n2023-07-27 22:25:31 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_sentence(text, remove_stop = False):\n    tokens = [token.strip().lower() for token in text.split() if not token in string.punctuation]\n    if remove_stop:\n        tokens = [token for token in tokens if not token in stops]\n    return \" \".join(tokens)\n\ndef get_sentence_embeddings_and_save(df, embeddings_index, embeddings_dim, output_path):\n    sentences = list(df['clean_sentence'].values)\n    embeddings = np.zeros((len(sentences), embeddings_dim))\n    \n    for i, sentence in tqdm(enumerate(sentences)):\n        sentence_embedding = np.zeros((embeddings_dim))\n#         tokens = nltk.word_tokenize(sentence)\n        tokens = sentence.split()\n        \n        found_tokens = 0\n        for token in tokens:\n            token_embedding = embeddings_index.get(token)\n            if token_embedding is not None:\n                # we found the word - add that word's vector to the sentence embedding\n                found_tokens += 1\n                sentence_embedding = np.sum((sentence_embedding, token_embedding), axis = 0)\n\n        # sentence embedding as average of token embeddings\n        if found_tokens == 0:\n            found_tokens = 1\n        sentence_embedding = sentence_embedding / found_tokens\n\n        # add the sentence embedding to the matrix at sentence position\n        embeddings[i] = sentence_embedding\n    \n    # save csv file with embeddings, set and original labels\n    emb_df = df.copy()\n    emb_df['embeddings'] = embeddings.tolist()\n    emb_df.to_csv(output_path, index = False)\n    print(f\"file saved {output_path}...\")\n\n# get glove embeddings from file\nembeddings_index, embeddings_dim = {}, 300\nwith open('/kaggle/working/glove.6B.300d.txt') as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\nprint('Found {} word vectors.'.format(len(embeddings_index)))\n\nfor df_name in ['pe', 'we', 'abam', 'ug', 'mix1']:\n    df_path = f\"/kaggle/input/unit-segmentation-lstm-transformers/{df_name}.csv\"\n    dataframe = pd.read_csv(df_path)\n    \n    dataframe['clean_sentence'] = dataframe['tokens'].apply(lambda x: clean_sentence(x))\n    \n    get_sentence_embeddings_and_save(dataframe, embeddings_index, embeddings_dim, f\"{df_name}_glove.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-07-27T22:27:07.977546Z","iopub.execute_input":"2023-07-27T22:27:07.977936Z","iopub.status.idle":"2023-07-27T22:28:30.908413Z","shell.execute_reply.started":"2023-07-27T22:27:07.977906Z","shell.execute_reply":"2023-07-27T22:28:30.907065Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 400000 word vectors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baf2aae997da43df98e8de5cac3a6b90"}},"metadata":{}},{"name":"stdout","text":"file saved pe_glove.csv...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78df526f5ace4cb78af391ce48a7cbea"}},"metadata":{}},{"name":"stdout","text":"file saved we_glove.csv...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf4318a00b8c4451affa2cbf1016d785"}},"metadata":{}},{"name":"stdout","text":"file saved abam_glove.csv...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ea20ac4ddd748a1bed6ef42a8438578"}},"metadata":{}},{"name":"stdout","text":"file saved ug_glove.csv...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48370dba67dd40a892eb9dc1cdab04d4"}},"metadata":{}},{"name":"stdout","text":"file saved mix1_glove.csv...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Sentence BERT","metadata":{}},{"cell_type":"code","source":"# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0].cpu()\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings / sum_mask\n\ndef get_sentence_embeddings_and_save(df, model, output_path):\n    \n    model.eval()\n    \n    sentences = list(df['tokens'].values)\n    \n    batch_size, le = 500, len(df)\n    embeddings = np.zeros((le, 384))\n    \n    for i in tqdm(range(0, le, batch_size)):\n    \n        batch_sentences = sentences[i:i+batch_size]\n\n        encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n        input_ids = encoded_input['input_ids'].to(device)\n        token_type_ids = encoded_input['token_type_ids'].to(device)\n        attention_mask = encoded_input['attention_mask'].to(device)\n        with torch.no_grad():\n            model_output = model(input_ids, token_type_ids = token_type_ids, attention_mask=attention_mask)\n\n        pooling = mean_pooling(model_output, encoded_input['attention_mask'])\n\n        embeddings[i:i+batch_size, :] = pooling.tolist()\n        \n    # save csv file with embeddings, set and original labels\n    emb_df = df.copy()\n    emb_df['embeddings'] = embeddings.tolist()\n    emb_df.to_csv(output_path, index = False)\n    print(f\"file saved {output_path}...\")\n\n    \n    \nfor df_name in ['pe', 'we', 'abam', 'ug', 'mix1']:\n    df_path = f\"/kaggle/input/unit-segmentation-lstm-transformers/{df_name}.csv\"\n    dataframe = pd.read_csv(df_path)\n    \n    #Load AutoModel from huggingface model repository\n    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n    model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n    model.to(device)\n    \n    get_sentence_embeddings_and_save(dataframe, model, f\"{df_name}_sbert.csv\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-27T21:50:20.803543Z","iopub.execute_input":"2023-07-27T21:50:20.804317Z","iopub.status.idle":"2023-07-27T21:51:48.562913Z","shell.execute_reply.started":"2023-07-27T21:50:20.804275Z","shell.execute_reply":"2023-07-27T21:51:48.561865Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"295545b65da242498b64bf74a6af15c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faeb13024da247a6bd41fb779d4e7078"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4edbadc8240648e7a06cda031d6bd580"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"008b5699ec6c4f79b574f4946f431155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baaf5f892fa04bfd9d2858920ee1443e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1ee9d4bfa8495ba1065d77b7b20479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e7e71083254eee8f0d81b1e7fbce45"}},"metadata":{}},{"name":"stdout","text":"file saved pe_sbert.csv...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90cc7f33529941a6aa0e38a405f1b30f"}},"metadata":{}},{"name":"stdout","text":"file saved we_sbert.csv...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4093d7fececd451c809f8ab2969b565b"}},"metadata":{}},{"name":"stdout","text":"file saved abam_sbert.csv...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e60fcbf87b34f5bb37dcddd47880bdb"}},"metadata":{}},{"name":"stdout","text":"file saved ug_sbert.csv...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/27 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"336e179b28d34b1199077871cfff6f28"}},"metadata":{}},{"name":"stdout","text":"file saved mix1_sbert.csv...\n","output_type":"stream"}]}]}