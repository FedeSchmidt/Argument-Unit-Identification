{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6071846,"sourceType":"datasetVersion","datasetId":2578400},{"sourceId":6074461,"sourceType":"datasetVersion","datasetId":1751136}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport collections\nimport time\nimport pathlib\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport random\n\nimport torch\nfrom torch import nn, cuda\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom scipy.stats import spearmanr, pearsonr\n\nimport transformers\nfrom transformers import BertModel, BertTokenizerFast, DistilBertModel, DistilBertTokenizerFast\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, ConcatDataset\nimport copy\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nif device == 'cuda':\n    torch.cuda.empty_cache()\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-07T12:33:28.104470Z","iopub.execute_input":"2023-07-07T12:33:28.104811Z","iopub.status.idle":"2023-07-07T12:33:44.112591Z","shell.execute_reply.started":"2023-07-07T12:33:28.104783Z","shell.execute_reply":"2023-07-07T12:33:44.111271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(DATA_PATH)\n\n# TASK = 'relnorel'\n# TASK = 'attsup'\nTASK = 'premiseclaim'\n\ndataframe = None\n\nif (TASK == 'relnorel') or (TASK == 'attsup'):\n    DATA_PATH = pathlib.Path('/kaggle/input/ukp-sentential-am-corpus')\n    dataframe = pd.read_csv(DATA_PATH / 'ukp-data.csv')\n    \n    if TASK == 'attsup':\n        dataframe = dataframe.loc[dataframe.label != 'NoArgument']    \nelif TASK == 'premiseclaim':\n    DATA_PATH = pathlib.Path('/kaggle/input/argumentannotatedessays')\n    dataframe = pd.read_csv(DATA_PATH / 'data.csv')\n    dataframe = dataframe[dataframe.label != 'majorclaim']\n    dataframe = dataframe.rename(columns={'text': 'sentence'})\n    \n    \n# if TASK == 'attsup':\n#     dataframe = dataframe.loc[dataframe.label != 'NoArgument']\n\nprint(len(dataframe.topic.unique()))\nprint(collections.Counter(dataframe.set))\nprint(collections.Counter(dataframe.label))\ndataframe.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:33:49.247437Z","iopub.execute_input":"2023-07-07T12:33:49.247873Z","iopub.status.idle":"2023-07-07T12:33:49.366200Z","shell.execute_reply.started":"2023-07-07T12:33:49.247841Z","shell.execute_reply":"2023-07-07T12:33:49.365362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nDataset class\n\"\"\"\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:34:06.184295Z","iopub.execute_input":"2023-07-07T12:34:06.184819Z","iopub.status.idle":"2023-07-07T12:34:06.193600Z","shell.execute_reply.started":"2023-07-07T12:34:06.184765Z","shell.execute_reply":"2023-07-07T12:34:06.192706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTClassifier(nn.Module):\n    def __init__(self, num_labels = 2):\n        super(BERTClassifier, self).__init__()\n        self.num_labels = num_labels\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n        self.classifier = nn.Linear(768, self.num_labels)\n        self.dropout = torch.nn.Dropout(0.1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        x = self.bert(input_ids, \n                      token_type_ids = token_type_ids,\n                      attention_mask = attention_mask)\n        hidden = self.dropout(nn.LeakyReLU()(x.pooler_output))\n        return self.classifier(hidden)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:34:08.047256Z","iopub.execute_input":"2023-07-07T12:34:08.047685Z","iopub.status.idle":"2023-07-07T12:34:08.056082Z","shell.execute_reply.started":"2023-07-07T12:34:08.047653Z","shell.execute_reply":"2023-07-07T12:34:08.055010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Establecer la semilla aleatoria\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef prepare_input(df, labels, tokenizer, max_len = 256, shuffle = True, bs = 32):\n    encodings = tokenizer(list(df['sentence'].values),\n                          list(df['topic'].values),\n                          truncation=True, \n                          padding='max_length', \n                          max_length=max_len)\n    dst = CustomDataset(encodings, labels)\n    dataloader = DataLoader(dst, batch_size = bs, shuffle = shuffle)\n    return dataloader\n\n# save model to disc\ndef save_model_state(model_state, PATH):\n    torch.save(model_state, PATH)\n    \n# load and returns a previously saved model\ndef load_model(PATH):\n    model = BERTClassifier()\n    model.load_state_dict(torch.load(PATH))\n    return model\n\ndef get_batch_predictions(model, batch, loss_fn = None):\n    # mini-batch predictions\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    input_type_ids = batch['token_type_ids'].to(device)\n    labels = batch['labels'].type(torch.LongTensor).to(device)  # BS x1\n            \n    # get predictions for the mini-batch\n    outputs = model(input_ids, attention_mask, input_type_ids) # outputs BS x 2\n            \n    # calculate mini-batch loss and accuracy\n    loss = None\n    if loss_fn is not None:\n        loss = loss_fn(outputs, labels)\n    \n    return outputs, labels, loss\n\n# returns number of correct predictions for a given batch of samples\ndef batch_hits(logits, labels):\n    return (torch.flatten((logits > 0.0).float()) == torch.flatten(labels)).sum().item()\n\ndef finetune_model(model, train_loader, validation_loader, loss_fn, optim, scheduler, epochs, n_samples, n_eval_samples):\n\n    best_model_state = copy.deepcopy(model.state_dict())\n    best_loss = float('inf')\n    progress_bar = tqdm(range(len(train_loader) * epochs))\n    \n    for epoch in range(1, epochs + 1):\n                \n        print(\"Epoch: {}/{}\".format(epoch, epochs))\n        start_time = time.time()\n        \n        ## TRAINING LOOP\n        start_time = time.time()\n        train_loss, train_hits = 0, 0\n        model.train()\n        \n        for i, batch in enumerate(train_loader, 0):\n            optim.zero_grad()\n            \n            outputs, labels, loss = get_batch_predictions(model, batch, loss_fn)\n            train_loss += loss.item()\n            train_hits += (torch.argmax(outputs, dim = 1) == labels).sum().item()\n            \n            # Optimization step\n            loss.backward()\n            \n            # gradient clipping\n            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n            \n            # update learning rates using scheduler\n            scheduler.step()\n            optim.step()\n            \n            progress_bar.update(1)\n          \n        # calculate average loss and accuracy\n        avg_train_loss = round(float(train_loss / n_samples), 4)\n        train_acc = round(float(train_hits / n_samples), 3)\n            \n        ## VALIDATION LOOP\n        model.eval()\n        eval_loss = 0\n        eval_predictions, eval_labels = np.array([], int), np.array([], int)\n        \n        with torch.no_grad():\n            for batch in validation_loader:\n                outputs, labels, loss = get_batch_predictions(model, batch, loss_fn)\n                eval_loss += loss.item()\n                \n                eval_predictions = np.concatenate((eval_predictions, torch.argmax(outputs, dim = 1).int().cpu().numpy()), axis = 0)\n                eval_labels = np.concatenate((eval_labels, labels.int().cpu().numpy()), axis = 0)\n                \n        # calculate average loss, accuracy and macro f1 score\n        avg_eval_loss = round(float(eval_loss / n_eval_samples), 4)\n        eval_acc = accuracy_score(eval_labels, eval_predictions)\n        eval_macro_f1 = f1_score(eval_labels, eval_predictions, average = 'macro')\n        \n        ## PRINT INFO\n        print(f\"Training   ==> Accuracy: {train_acc} | Loss: {avg_train_loss}\")\n        print(f\"Validation ==> Accuracy: {eval_acc}  | Loss: {avg_eval_loss}  | Macro-F1: {eval_macro_f1}\")\n        \n        ## SAVE BEST MODEL\n        ## Save model state when validation loss decreases\n        if eval_loss < best_loss:\n            best_model_state = copy.deepcopy(model.state_dict())\n            best_loss = eval_loss\n            print(\"Validation loss has decreased... saving model at epoch {}\".format(epoch))\n    \n    return best_model_state\n\ndef test_model(model_state, dataloader, get_logits = False, classes = None, verbose = True):\n    \n    model = BERTClassifier()\n    model.load_state_dict(model_state)\n    model.to(device)\n    model.eval()\n    \n    predictions, test_labels = np.array([], int), np.array([], int)\n    logits = np.zeros((0, 2), dtype=np.float32)\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            outputs, labels, loss = get_batch_predictions(model, batch)\n            \n            logits = np.concatenate((logits, outputs.cpu().numpy()), axis = 0)\n            predictions = np.concatenate((predictions, torch.argmax(outputs, dim = 1).int().cpu().numpy()), axis = 0)\n            test_labels = np.concatenate((test_labels, labels.int().cpu().numpy()), axis = 0)\n            \n    if verbose:\n        if classes is not None:\n            print(classification_report(test_labels, predictions, target_names = classes))\n        else:\n            print(classification_report(test_labels, predictions))\n    \n    if get_logits:\n        return predictions, logits\n    else:\n        return predictions, None","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:34:16.481265Z","iopub.execute_input":"2023-07-07T12:34:16.482274Z","iopub.status.idle":"2023-07-07T12:34:16.515009Z","shell.execute_reply.started":"2023-07-07T12:34:16.482233Z","shell.execute_reply":"2023-07-07T12:34:16.513753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finetuning","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 256\nEPOCHS = 4\nEXPERIMENTS = 5\nLR = 2e-5\n\nSAVE_MODELS = True\n\ndf = dataframe.copy() # copy of dataframe\n    \nfor exp in range(EXPERIMENTS):\n    \n    # Establecer la semilla aleatoria para cada corrida\n    seed = exp*2023 + 1\n    set_seed(seed)\n    \n    # TOKENIZER\n    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n    # SEPARATE DATA\n    X_train = df.loc[df.set == 'train'].sample(frac = 1).reset_index(drop = True)\n#     X_val = df.loc[df.set == 'val'].sample(frac = 1).reset_index(drop = True)\n    X_test = df.loc[df.set == 'test'].reset_index(drop = True)\n    if TASK == 'premiseclaim':\n        X_train, X_val = train_test_split(X_train, test_size=0.15, random_state=42)\n    else:\n        X_val = df.loc[df.set == 'val'].sample(frac = 1).reset_index(drop = True)\n    \n    train_labels, val_labels, test_labels = None, None, None\n    \n    print(collections.Counter(X_train.label))\n    print(collections.Counter(X_val.label))\n    print(collections.Counter(X_test.label))\n    break\n    \n    if TASK == 'relnorel':\n        train_labels = np.asarray([1 if x != 'NoArgument' else 0 for x in X_train.label.values])\n        val_labels = np.asarray([1 if x != 'NoArgument' else 0 for x in X_val.label.values])\n        test_labels = np.asarray([1 if x != 'NoArgument' else 0 for x in X_test.label.values])\n        \n    elif TASK == 'attsup':\n        train_labels = np.asarray([1 if x != 'Argument_against' else 0 for x in X_train.label.values])\n        val_labels = np.asarray([1 if x != 'Argument_against' else 0 for x in X_val.label.values])\n        test_labels = np.asarray([1 if x != 'Argument_against' else 0 for x in X_test.label.values])\n        \n    elif TASK == 'premiseclaim':\n        train_labels = np.asarray([1 if x == 'claim' else 0 for x in X_train.label.values])\n        val_labels = np.asarray([1 if x == 'claim' else 0 for x in X_val.label.values])\n        test_labels = np.asarray([1 if x == 'claim' else 0 for x in X_test.label.values])\n        \n    else:\n        print(\"Invalid Task\")\n        break\n\n    train_samples = len(train_labels)\n    eval_samples = len(val_labels)\n    \n    # TRAINING DATA\n    train_loader = prepare_input(X_train, train_labels, tokenizer, bs = 32, shuffle = True)\n    validation_loader = prepare_input(X_val, val_labels, tokenizer, bs = 32, shuffle = True)\n    test_loader = prepare_input(X_test, test_labels, tokenizer, bs = 32, shuffle = False)\n    \n    # INITIALIZE MODEL\n    model = BERTClassifier()\n    model.to(device)\n    \n    # OPTIMIZER\n    optim = torch.optim.AdamW(model.parameters(), lr=LR)\n    \n    ### Scheduler\n    num_train_optimization_steps = EPOCHS * len(train_loader)\n    print(num_train_optimization_steps)\n    scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=300, num_training_steps=num_train_optimization_steps)\n    \n    # LOSS FUNCTION\n    loss_fn = nn.CrossEntropyLoss()    \n    \n    # training model\n    best_model_state = finetune_model(model, \n                                train_loader, \n                                validation_loader, \n                                loss_fn, \n                                optim, \n                                scheduler, \n                                EPOCHS, \n                                train_samples, \n                                eval_samples)\n    \n    test_predictions, test_logits = test_model(best_model_state, test_loader, get_logits = True)\n    test_logits = torch.from_numpy(test_logits)\n    softmax_tensor = F.softmax(test_logits, dim=1)\n    df_res = X_test.copy()\n    \n    if TASK == 'relnorel':\n        df_res['prediction'] = ['arg' if x == 1 else 'noarg' for x in test_predictions]\n        \n    elif TASK == 'attsup':\n        df_res['prediction'] = ['against' if x == 1 else 'for' for x in test_predictions]\n        \n    elif TASK == 'premiseclaim':\n        df_res['prediction'] = ['claim' if x == 1 else 'premise' for x in test_predictions]\n    \n    df_res['logits'] = [list(row) for row in softmax_tensor.numpy()]\n    \n    if TASK != 'premiseclaim':\n        df_res.to_csv(f\"binary_{TASK}_ukp_exp{exp}.csv\", index = False)\n    else:\n        df_res.to_csv(f\"binary_{TASK}_pe_exp{exp}.csv\", index = False)\n    \n    if SAVE_MODELS:\n        # saving the best model\n        best_model_name = f'trained_model_ukp_{TASK}_exp{exp}.pt'\n        if TASK == 'premiseclaim':\n            best_model_name = f'trained_model_pe_{TASK}_exp{exp}.pt'\n        save_model_state(best_model_state, best_model_name)\n        print(\"Best model saved... {}\".format(best_model_name))","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:35:25.527636Z","iopub.execute_input":"2023-07-07T12:35:25.528072Z","iopub.status.idle":"2023-07-07T12:35:25.697761Z","shell.execute_reply.started":"2023-07-07T12:35:25.528041Z","shell.execute_reply":"2023-07-07T12:35:25.696309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inferencia con dataset con perturbaciones","metadata":{}},{"cell_type":"code","source":"def prepare_input_for_test(df, column_name, labels, tokenizer, max_len = 256, shuffle = True, bs = 32):\n    encodings = tokenizer(list(df[column_name].values),\n                          list(df['topic'].values),\n                          truncation=True, \n                          padding='max_length', \n                          max_length=max_len)\n    dst = CustomDataset(encodings, labels)\n    dataloader = DataLoader(dst, batch_size = bs, shuffle = shuffle)\n    return dataloader\n\nDATA_PATH = pathlib.Path('/kaggle/input/ukp-sentential-am-corpus')\nprint(DATA_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T00:35:40.915383Z","iopub.execute_input":"2023-07-04T00:35:40.916534Z","iopub.status.idle":"2023-07-04T00:35:40.924329Z","shell.execute_reply.started":"2023-07-04T00:35:40.916500Z","shell.execute_reply":"2023-07-04T00:35:40.923393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instancias C1","metadata":{}},{"cell_type":"code","source":"# TASK = 'relnorel'\n# TASK = 'attsup'\nTASK = 'premiseclaim'\n\ndataframe = pd.read_csv(DATA_PATH / 'data_for_classification_C1_with_category.csv')\n\nif TASK == 'attsup':\n    dataframe = dataframe.loc[dataframe.relTag != 'NoArgument']\n\nprint(len(dataframe.topic.unique()))\n\nX_test = dataframe\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# load model\n# ...\n# model_for_inference = ...\nbest_experiment = 0\nif TASK == 'premiseclaim':\n    BEST_MODEL_PATH = f'/kaggle/working/trained_model_pe_{TASK}_exp{best_experiment}.pt'\nelse:\n    BEST_MODEL_PATH = f'/kaggle/working/trained_model_ukp_{TASK}_exp{best_experiment}.pt'\n\nmodel_state_for_inference = torch.load(BEST_MODEL_PATH)\n\n################################################\n############ Predictions sobre gold ############\n################################################\n\nif TASK == 'relnorel':\n    test_labels = np.asarray([1 if x != 'NoArgument' else 0 for x in X_test.relTag.values])       \nelif TASK == 'attsup':\n    test_labels = np.asarray([1 if x != 'Argument_against' else 0 for x in X_test.stanceTag.values])\nelif TASK == 'premiseclaim':\n    test_labels = np.asarray([1 for x in X_test.relTag.values])\nelse:\n    print(\"Invalid Task\")\n\ntest_loader = prepare_input_for_test(X_test, 'goldArgument', test_labels, tokenizer, bs = 32, shuffle = False)\n\ntest_predictions, test_logits = test_model(model_state_for_inference, test_loader, get_logits = True)\n\ntest_logits = torch.from_numpy(test_logits)\nsoftmax_tensor = F.softmax(test_logits, dim=1)\ndf_res = X_test.copy()\n    \nif TASK == 'relnorel':\n    df_res['prediction'] = ['arg' if x == 1 else 'noarg' for x in test_predictions]\nelif TASK == 'attsup':\n    df_res['prediction'] = ['against' if x == 1 else 'for' for x in test_predictions]\nelif TASK == 'premiseclaim':\n    df_res['prediction'] = ['claim' if x == 1 else 'premise' for x in test_predictions]\n    \ndf_res['logits'] = [list(row) for row in softmax_tensor.numpy()]\ndf_res.to_csv(f\"binary_{TASK}_c1_gold.csv\", index = False)\n\n################################################\n#### Predictions sobre instancias con error ####\n################################################\n\nif TASK == 'relnorel':\n    test_labels = np.asarray([1 if x != 'NoArgument' else 0 for x in X_test.relTag.values])       \nelif TASK == 'attsup':\n    test_labels = np.asarray([1 if x != 'Argument_against' else 0 for x in X_test.stanceTag.values])\nelif TASK == 'premiseclaim':\n    test_labels = np.asarray([1 for x in X_test.relTag.values])\nelse:\n    print(\"Invalid Task\")\n\ntest_loader = prepare_input_for_test(X_test, 'predictedArgument', test_labels, tokenizer, bs = 32, shuffle = False)\n\ntest_predictions, test_logits = test_model(model_state_for_inference, test_loader, get_logits = True)\n\ntest_logits = torch.from_numpy(test_logits)\nsoftmax_tensor = F.softmax(test_logits, dim=1)\ndf_res = X_test.copy()\n    \nif TASK == 'relnorel':\n    df_res['prediction'] = ['arg' if x == 1 else 'noarg' for x in test_predictions]\nelif TASK == 'attsup':\n    df_res['prediction'] = ['against' if x == 1 else 'for' for x in test_predictions]\nelif TASK == 'premiseclaim':\n    df_res['prediction'] = ['claim' if x == 1 else 'premise' for x in test_predictions]\n    \ndf_res['logits'] = [list(row) for row in softmax_tensor.numpy()]\ndf_res.to_csv(f\"binary_{TASK}_c1_with_errors.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T00:36:14.589634Z","iopub.execute_input":"2023-07-04T00:36:14.590050Z","iopub.status.idle":"2023-07-04T00:38:17.614770Z","shell.execute_reply.started":"2023-07-04T00:36:14.590014Z","shell.execute_reply":"2023-07-04T00:38:17.613783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instancias C2","metadata":{}},{"cell_type":"code","source":"# TASK = 'relnorel'\n# TASK = 'attsup'\nTASK = 'premiseclaim'\n\ndataframe = pd.read_csv(DATA_PATH / 'data_for_classification_C2_with_category.csv')\n\nprint(len(dataframe.topic.unique()))\n\nX_test = dataframe\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# load model\n# ...\n# model_for_inference = ...\nbest_experiment = 0\nif TASK == 'premiseclaim':\n    BEST_MODEL_PATH = f'/kaggle/working/trained_model_pe_{TASK}_exp{best_experiment}.pt'\nelse:\n    BEST_MODEL_PATH = f'/kaggle/working/trained_model_ukp_{TASK}_exp{best_experiment}.pt'\n\nmodel_state_for_inference = torch.load(BEST_MODEL_PATH)\n\n################################################\n#### Predictions sobre instancias con error ####\n################################################\n\nif TASK == 'relnorel':\n    test_labels = np.asarray([1 if x != 'NoArgument' else 0 for x in X_test.relTag.values])       \nelif TASK == 'attsup':\n    test_labels = np.asarray([1 if x != 'Argument_against' else 0 for x in X_test.stanceTag.values])\nelif TASK == 'premiseclaim':\n    test_labels = np.asarray([1 for x in X_test.relTag.values])\nelse:\n    print(\"Invalid Task\")\n\ntest_loader = prepare_input_for_test(X_test, 'predictedArgument', test_labels, tokenizer, bs = 32, shuffle = False)\n\ntest_predictions, test_logits = test_model(model_state_for_inference, test_loader, get_logits = True)\n\ntest_logits = torch.from_numpy(test_logits)\nsoftmax_tensor = F.softmax(test_logits, dim=1)\ndf_res = X_test.copy()\n    \nif TASK == 'relnorel':\n    df_res['prediction'] = ['arg' if x == 1 else 'noarg' for x in test_predictions]\nelif TASK == 'attsup':\n    df_res['prediction'] = ['against' if x == 1 else 'for' for x in test_predictions]\nelif TASK == 'premiseclaim':\n    df_res['prediction'] = ['claim' if x == 1 else 'premise' for x in test_predictions]\n    \ndf_res['logits'] = [list(row) for row in softmax_tensor.numpy()]\ndf_res.to_csv(f\"binary_{TASK}_c2_with_errors.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T00:38:22.753178Z","iopub.execute_input":"2023-07-04T00:38:22.754305Z","iopub.status.idle":"2023-07-04T00:38:31.343598Z","shell.execute_reply.started":"2023-07-04T00:38:22.754254Z","shell.execute_reply":"2023-07-04T00:38:31.342531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instancias C3 y C4","metadata":{}},{"cell_type":"code","source":"# TASK = 'relnorel'\n# TASK = 'attsup'\nTASK = 'premiseclaim'\n\ndataframe = pd.read_csv(DATA_PATH / 'data_for_classification_C34_with_category.csv')\n\nprint(len(dataframe.topic.unique()))\n\nX_test = dataframe\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# load model\n# ...\n# model_for_inference = ...\nbest_experiment = 0\n# BEST_MODEL_PATH = f'/kaggle/working/trained_model_ukp_{TASK}_exp{best_experiment}.pt'\nif TASK == 'premiseclaim':\n    BEST_MODEL_PATH = f'/kaggle/working/trained_model_pe_{TASK}_exp{best_experiment}.pt'\nelse:\n    BEST_MODEL_PATH = f'/kaggle/working/trained_model_ukp_{TASK}_exp{best_experiment}.pt'\nmodel_state_for_inference = torch.load(BEST_MODEL_PATH)\n\n################################################\n#### Predictions sobre instancias con error ####\n################################################\n\nif TASK == 'relnorel':\n    test_labels = np.asarray([1 if x != 'NoArgument' else 0 for x in X_test.relTag.values])       \nelif TASK == 'attsup':\n    test_labels = np.asarray([1 if x != 'Argument_against' else 0 for x in X_test.stanceTag.values])\nelif TASK == 'premiseclaim':\n    test_labels = np.asarray([1 for x in X_test.stanceTag.values])\nelse:\n    print(\"Invalid Task\")\n\ntest_loader = prepare_input_for_test(X_test, 'predictedArgument', test_labels, tokenizer, bs = 32, shuffle = False)\n\ntest_predictions, test_logits = test_model(model_state_for_inference, test_loader, get_logits = True)\n\ntest_logits = torch.from_numpy(test_logits)\nsoftmax_tensor = F.softmax(test_logits, dim=1)\ndf_res = X_test.copy()\n    \nif TASK == 'relnorel':\n    df_res['prediction'] = ['arg' if x == 1 else 'noarg' for x in test_predictions]\nelif TASK == 'attsup':\n    df_res['prediction'] = ['against' if x == 1 else 'for' for x in test_predictions]\nelif TASK == 'premiseclaim':\n    df_res['prediction'] = ['claim' if x == 1 else 'premise' for x in test_predictions]\n    \ndf_res['logits'] = [list(row) for row in softmax_tensor.numpy()]\ndf_res.to_csv(f\"binary_{TASK}_c34_with_errors.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T00:38:35.662676Z","iopub.execute_input":"2023-07-04T00:38:35.663063Z","iopub.status.idle":"2023-07-04T00:40:39.574594Z","shell.execute_reply.started":"2023-07-04T00:38:35.663031Z","shell.execute_reply":"2023-07-04T00:40:39.572903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instancias C1 SPLIT","metadata":{}},{"cell_type":"code","source":"# TASK = 'relnorel'\n# TASK = 'attsup'\nTASK = 'premiseclaim'\n\ndataframe = pd.read_csv(DATA_PATH / 'data_for_classification_C1_SPLIT_with_category.csv')\n\ndataframe['predictedArgument1'] = dataframe['predictedArgument1'].fillna('')\ndataframe['predictedArgument2'] = dataframe['predictedArgument2'].fillna('')\n\nprint(len(dataframe.topic.unique()))\n\nX_test = dataframe\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# load model\n# ...\n# model_for_inference = ...\nbest_experiment = 0\n# BEST_MODEL_PATH = f'/kaggle/working/trained_model_ukp_{TASK}_exp{best_experiment}.pt'\nif TASK == 'premiseclaim':\n    BEST_MODEL_PATH = f'/kaggle/working/trained_model_pe_{TASK}_exp{best_experiment}.pt'\nelse:\n    BEST_MODEL_PATH = f'/kaggle/working/trained_model_ukp_{TASK}_exp{best_experiment}.pt'\n\nmodel_state_for_inference = torch.load(BEST_MODEL_PATH)\n\n################################################\n############ Predictions sobre gold ############\n################################################\n\nif TASK == 'relnorel':\n    test_labels = np.asarray([1 if x != 'NoArgument' else 0 for x in X_test.relTag.values])       \nelif TASK == 'attsup':\n    test_labels = np.asarray([1 if x != 'Argument_against' else 0 for x in X_test.stanceTag.values])\nelif TASK == 'premiseclaim':\n    test_labels = np.asarray([1 for x in X_test.stanceTag.values])\nelse:\n    print(\"Invalid Task\")\n\ntest_loader = prepare_input_for_test(X_test, 'goldArgument', test_labels, tokenizer, bs = 32, shuffle = False)\n\ntest_predictions, test_logits = test_model(model_state_for_inference, test_loader, get_logits = True)\n\ntest_logits = torch.from_numpy(test_logits)\nsoftmax_tensor = F.softmax(test_logits, dim=1)\ndf_res = X_test.copy()\n    \nif TASK == 'relnorel':\n    df_res['prediction'] = ['arg' if x == 1 else 'noarg' for x in test_predictions]\nelif TASK == 'attsup':\n    df_res['prediction'] = ['against' if x == 1 else 'for' for x in test_predictions]\nelif TASK == 'premiseclaim':\n    df_res['prediction'] = ['claim' if x == 1 else 'premise' for x in test_predictions]\n    \ndf_res['logits'] = [list(row) for row in softmax_tensor.numpy()]\ndf_res.to_csv(f\"binary_{TASK}_c1_SPLIT_gold.csv\", index = False)\n\n################################################\n#### Predictions sobre instancias con error (SPLIT 1) ####\n################################################\n\nif TASK == 'relnorel':\n    test_labels = np.asarray([1 if x != 'NoArgument' else 0 for x in X_test.relTag.values])       \nelif TASK == 'attsup':\n    test_labels = np.asarray([1 if x != 'Argument_against' else 0 for x in X_test.stanceTag.values])\nelif TASK == 'premiseclaim':\n    test_labels = np.asarray([1 for x in X_test.stanceTag.values])\nelse:\n    print(\"Invalid Task\")\n\ntest_loader = prepare_input_for_test(X_test, 'predictedArgument1', test_labels, tokenizer, bs = 32, shuffle = False)\n\ntest_predictions, test_logits = test_model(model_state_for_inference, test_loader, get_logits = True)\n\ntest_logits = torch.from_numpy(test_logits)\nsoftmax_tensor = F.softmax(test_logits, dim=1)\ndf_res = X_test.copy()\n    \nif TASK == 'relnorel':\n    df_res['prediction1'] = ['arg' if x == 1 else 'noarg' for x in test_predictions]\nelif TASK == 'attsup':\n    df_res['prediction1'] = ['against' if x == 1 else 'for' for x in test_predictions]\nelif TASK == 'premiseclaim':\n    df_res['prediction1'] = ['claim' if x == 1 else 'premise' for x in test_predictions]\n    \ndf_res['logits1'] = [list(row) for row in softmax_tensor.numpy()]\n\n# ---------------\ntest_loader = prepare_input_for_test(X_test, 'predictedArgument2', test_labels, tokenizer, bs = 32, shuffle = False)\ntest_predictions, test_logits = test_model(model_state_for_inference, test_loader, get_logits = True)\n\ntest_logits = torch.from_numpy(test_logits)\nsoftmax_tensor = F.softmax(test_logits, dim=1)\n    \nif TASK == 'relnorel':\n    df_res['prediction2'] = ['arg' if x == 1 else 'noarg' for x in test_predictions]\nelif TASK == 'attsup':\n    df_res['prediction2'] = ['against' if x == 1 else 'for' for x in test_predictions]\nelif TASK == 'premiseclaim':\n    df_res['prediction2'] = ['claim' if x == 1 else 'premise' for x in test_predictions]\n    \ndf_res['logits2'] = [list(row) for row in softmax_tensor.numpy()]\n\n# ------------------\n\ndf_res.to_csv(f\"binary_{TASK}_c1_SPLIT_with_errors.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T00:40:45.059417Z","iopub.execute_input":"2023-07-04T00:40:45.059768Z","iopub.status.idle":"2023-07-04T00:43:49.046779Z","shell.execute_reply.started":"2023-07-04T00:40:45.059738Z","shell.execute_reply":"2023-07-04T00:43:49.045730Z"},"trusted":true},"execution_count":null,"outputs":[]}]}